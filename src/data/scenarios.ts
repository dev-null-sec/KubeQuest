/**
 * å‰§æƒ…æ¨¡å¼å…³å¡å®šä¹‰
 */

import type { ClusterState } from '../engine/cluster';
import { ckaScenarios } from './cka';

export interface ScenarioObjective {
    id: string;
    description: string;
    hint?: string;
    checkCondition: (state: ClusterState, commandHistory: string[]) => boolean;
    completed?: boolean;
}

export interface Scenario {
    id: string;
    title: string;
    description: string;
    story: string;
    difficulty: 'easy' | 'medium' | 'hard';
    objectives: ScenarioObjective[];
    initialState?: Partial<ClusterState>;
    initialFiles?: Record<string, string>; // è·¯å¾„ -> å†…å®¹ çš„æ˜ å°„
    hints: string[];
    rewards: {
        xp: number;
        title?: string;
        badges?: string[];
    };
}

// ========== ç¬¬ä¸€ç« ï¼šåŸºç¡€æ“ä½œ ==========

const scenario1_1: Scenario = {
    id: '1-1',
    title: 'åˆå…¥é›†ç¾¤',
    description: 'ç†Ÿæ‚‰ kubectl åŸºæœ¬å‘½ä»¤',
    story: `æ¬¢è¿ä½¿ç”¨ Kubernetes æ¨¡æ‹Ÿå™¨ï¼

ä½ ç°åœ¨åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„Kubernetesç¯å¢ƒä¸­ã€‚è®©æˆ‘ä»¬ä»æœ€åŸºæœ¬çš„æ“ä½œå¼€å§‹ï¼Œå…ˆæ£€æŸ¥ä¸€ä¸‹é›†ç¾¤çš„èŠ‚ç‚¹çŠ¶æ€ã€‚

æ‰§è¡Œ kubectl get nodes å‘½ä»¤æ¥æŸ¥çœ‹é›†ç¾¤ä¸­çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚`,
    difficulty: 'easy',
    objectives: [
        {
            id: 'get-nodes',
            description: 'ä½¿ç”¨ kubectl get nodes æŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹',
            hint: 'å°è¯•è¿è¡Œï¼škubectl get nodes',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(nodes?|no)/.test(cmd))
        },
        {
            id: 'get-pods',
            description: 'ä½¿ç”¨ kubectl get pods æŸ¥çœ‹é»˜è®¤å‘½åç©ºé—´çš„ Pod',
            hint: 'å°è¯•è¿è¡Œï¼škubectl get pods',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd))
        },
        {
            id: 'get-namespaces',
            description: 'æŸ¥çœ‹é›†ç¾¤ä¸­æ‰€æœ‰çš„å‘½åç©ºé—´',
            hint: 'å°è¯•è¿è¡Œï¼škubectl get namespaces',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(namespaces?|ns)/.test(cmd))
        }
    ],
    hints: [
        'ä½¿ç”¨ kubectl get <èµ„æºç±»å‹> å¯ä»¥åˆ—å‡ºèµ„æº',
        'kubectl get nodes æ˜¾ç¤ºé›†ç¾¤èŠ‚ç‚¹',
        'kubectl get pods æ˜¾ç¤º Pod åˆ—è¡¨',
        'kubectl get ns æ˜¯ kubectl get namespaces çš„ç®€å†™'
    ],
    rewards: { xp: 50 }
};

const scenario1_2: Scenario = {
    id: '1-2',
    title: 'åˆ›å»ºç¬¬ä¸€ä¸ª Pod',
    description: 'å­¦ä¹ åˆ›å»º Pod',
    story: `"ä¸é”™ï¼ç¯å¢ƒæ£€æŸ¥å®Œæ¯•ã€‚" Leader æ»¡æ„åœ°ç‚¹ç‚¹å¤´ã€‚

"æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦éƒ¨ç½²ä¸€ä¸ªæµ‹è¯•ç”¨çš„ Nginx æœåŠ¡å™¨ã€‚ä½ å…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„ Pod å§ã€‚"

ä½ æ‰“å¼€ç»ˆç«¯ï¼Œå‡†å¤‡å¼€å§‹ä½ çš„ç¬¬ä¸€æ¬¡éƒ¨ç½²...`,
    difficulty: 'easy',
    objectives: [
        {
            id: 'create-nginx-pod',
            description: 'åˆ›å»ºä¸€ä¸ªä½¿ç”¨ nginx é•œåƒçš„ Podï¼ˆåç§°ä»»æ„ï¼‰',
            hint: 'ä½¿ç”¨ kubectl run æˆ–åˆ›å»º YAML æ–‡ä»¶å kubectl apply -f',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.containers.some(c => c.image.includes('nginx'))
            )
        },
        {
            id: 'verify-pod-running',
            description: 'ä½¿ç”¨ kubectl get pods ç¡®è®¤ Pod çŠ¶æ€ä¸º Running',
            hint: 'è¿è¡Œ kubectl get pods æŸ¥çœ‹çŠ¶æ€',
            checkCondition: (state, history) => 
                history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd)) &&
                state.pods.some(p => 
                    p.spec.containers.some(c => c.image.includes('nginx')) && 
                    p.status.phase === 'Running'
                )
        }
    ],
    hints: [
        'æ–¹å¼1: kubectl run my-nginx --image=nginx',
        'æ–¹å¼2: å…ˆ kubectl run xxx --image=nginx --dry-run=client -o yaml > pod.yamlï¼Œå† kubectl apply -f pod.yaml',
        'æœ€åè¿è¡Œ kubectl get pods ç¡®è®¤çŠ¶æ€'
    ],
    rewards: { xp: 75 }
};

const scenario1_3: Scenario = {
    id: '1-3',
    title: 'Pod çš„ç”Ÿå‘½å‘¨æœŸ',
    description: 'å­¦ä¹ æŸ¥çœ‹ã€åˆ é™¤å’Œç®¡ç† Pod',
    story: `"å¤ªæ£’äº†ï¼Pod å·²ç»è¿è¡Œèµ·æ¥äº†ã€‚" Leader èµ°è¿‡æ¥æŸ¥çœ‹ä½ çš„å±å¹•ã€‚

"ç°åœ¨è®©æˆ‘æ•™ä½ ä¸€äº›æ›´é«˜çº§çš„æ“ä½œã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ª Pod çš„è¯¦ç»†ä¿¡æ¯ï¼Œç„¶ååˆ é™¤å®ƒã€‚"

æ³¨æ„ï¼šé›†ç¾¤ä¸­å·²ç»æœ‰ä¸€ä¸ªåä¸º test-nginx çš„ Pod ä¾›ä½ æ“ä½œã€‚`,
    difficulty: 'easy',
    initialState: {
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'test-nginx', namespace: 'default', labels: { app: 'nginx' } },
                spec: { 
                    containers: [{ name: 'nginx', image: 'nginx:alpine' }],
                    restartPolicy: 'Always'
                },
                status: { 
                    phase: 'Running',
                    containerStatuses: [{
                        name: 'nginx',
                        ready: true,
                        restartCount: 0,
                        state: { running: { startedAt: new Date().toISOString() } },
                        image: 'nginx:alpine',
                        imageID: 'docker-pullable://nginx@sha256:abc123'
                    }]
                }
            }
        ]
    },
    objectives: [
        {
            id: 'describe-pod',
            description: 'ä½¿ç”¨ kubectl describe pod test-nginx æŸ¥çœ‹ Pod è¯¦ç»†ä¿¡æ¯',
            hint: 'è¿è¡Œï¼škubectl describe pod test-nginx',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+describe\s+pod\s+test-nginx/.test(cmd))
        },
        {
            id: 'delete-pod',
            description: 'åˆ é™¤ test-nginx Pod',
            hint: 'è¿è¡Œï¼škubectl delete pod test-nginx',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+delete\s+pod\s+test-nginx/.test(cmd))
        },
        {
            id: 'verify-deleted',
            description: 'ä½¿ç”¨ kubectl get pods ç¡®è®¤ Pod å·²è¢«åˆ é™¤',
            hint: 'è¿è¡Œ kubectl get pods æŸ¥çœ‹',
            checkCondition: (state, history) => 
                history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd)) &&
                !state.pods.some(p => p.metadata.name === 'test-nginx')
        }
    ],
    hints: [
        'kubectl describe pod test-nginx æ˜¾ç¤º Pod è¯¦ç»†ä¿¡æ¯',
        'kubectl delete pod test-nginx åˆ é™¤æŒ‡å®š Pod',
        'åˆ é™¤åè¿è¡Œ kubectl get pods ç¡®è®¤'
    ],
    rewards: { xp: 75 }
};

// ========== ç¬¬äºŒç« ï¼šå·¥ä½œè´Ÿè½½ç®¡ç† ==========

const scenario2_1: Scenario = {
    id: '2-1',
    title: 'Deployment ä¸å‰¯æœ¬è‡ªæ„ˆ',
    description: 'ä½¿ç”¨ Deployment ç®¡ç†åº”ç”¨å‰¯æœ¬ï¼Œç†è§£æ§åˆ¶å™¨çš„è‡ªæ„ˆæœºåˆ¶',
    story: `ä¸€å‘¨åï¼Œä½ å·²ç»ç†Ÿæ‚‰äº†åŸºæœ¬æ“ä½œã€‚ä»Šå¤©æ”¶åˆ°äº†ä¸€ä¸ªæ–°ä»»åŠ¡ï¼š

"æˆ‘ä»¬éœ€è¦éƒ¨ç½²ä¸€ä¸ªç”Ÿäº§çº§åˆ«çš„ Web åº”ç”¨ã€‚" é¡¹ç›®ç»ç†è¯´é“ï¼Œ"è¦ç¡®ä¿é«˜å¯ç”¨ï¼Œå³ä½¿æŸä¸ª Pod æŒ‚äº†ï¼ŒæœåŠ¡ä¹Ÿä¸èƒ½ä¸­æ–­ã€‚"

"Deployment æ§åˆ¶å™¨ä¼šè‡ªåŠ¨ç»´æŠ¤æœŸæœ›çš„å‰¯æœ¬æ•°ï¼Œ" ä½ è§£é‡Šé“ï¼Œ"å¦‚æœæŸä¸ª Pod è¢«åˆ é™¤æˆ–å´©æºƒï¼Œå®ƒä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„ Pod æ¥æ›¿ä»£ã€‚"

"å¬èµ·æ¥ä¸é”™ï¼Œæ¼”ç¤ºä¸€ä¸‹ï¼"

ğŸ’¡ æç¤ºï¼šDeployment æ˜¯å£°æ˜å¼çš„ï¼Œå®ƒä¼šæŒç»­åè°ƒå®é™…çŠ¶æ€ä¸æœŸæœ›çŠ¶æ€çš„å·®å¼‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-deployment',
            description: 'åˆ›å»ºä¸€ä¸ª Deploymentï¼Œè‡³å°‘ 3 ä¸ªå‰¯æœ¬',
            hint: 'ä½¿ç”¨ kubectl create deployment æˆ– YAML',
            checkCondition: (state) => state.deployments.some(d => d.spec.replicas >= 3)
        },
        {
            id: 'verify-deployment',
            description: 'ä½¿ç”¨ kubectl get deploy ç¡®è®¤ Deployment çŠ¶æ€',
            hint: 'æŸ¥çœ‹ READY åˆ—æ˜¯å¦æ˜¾ç¤º 3/3',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(deployments?|deploy)/.test(cmd)
            )
        },
        {
            id: 'verify-pods',
            description: 'æŸ¥çœ‹ Pod åˆ—è¡¨ï¼Œç¡®è®¤ 3 ä¸ªå‰¯æœ¬éƒ½åœ¨è¿è¡Œ',
            hint: 'ä½¿ç”¨ kubectl get pods æŸ¥çœ‹',
            checkCondition: (state, history) => 
                history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd)) &&
                state.pods.filter(p => p.status.phase === 'Running').length >= 3
        },
        {
            id: 'test-self-healing',
            description: 'åˆ é™¤ä¸€ä¸ª Podï¼Œè§‚å¯Ÿ Deployment è‡ªåŠ¨æ¢å¤ï¼ˆä¼šåˆ›å»ºæ–° Podï¼‰',
            hint: 'kubectl delete pod <pod-name>ï¼Œç„¶åå† get pods è§‚å¯Ÿ',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+delete\s+pod/.test(cmd))
        },
        {
            id: 'scale-deployment',
            description: 'å°† Deployment æ‰©å®¹åˆ° 5 ä¸ªå‰¯æœ¬',
            hint: 'kubectl scale deployment <name> --replicas=5',
            checkCondition: (state) => state.deployments.some(d => d.spec.replicas >= 5)
        }
    ],
    hints: [
        'kubectl create deployment myapp --image=nginx --replicas=3',
        'kubectl get deploy æŸ¥çœ‹ Deployment çŠ¶æ€',
        'åˆ é™¤ Pod åï¼ŒDeployment æ§åˆ¶å™¨ä¼šè‡ªåŠ¨åˆ›å»ºæ–° Pod ç»´æŒæœŸæœ›å‰¯æœ¬æ•°',
        'kubectl scale deployment <name> --replicas=5 æ‰©å®¹'
    ],
    rewards: { xp: 120 }
};

const scenario2_2: Scenario = {
    id: '2-2',
    title: 'Service æš´éœ²æœåŠ¡',
    description: 'ä½¿ç”¨ Service æš´éœ²åº”ç”¨å¹¶ç†è§£ Label Selector',
    story: `"Deployment éƒ¨ç½²æˆåŠŸäº†ï¼" ä½ å‘å›¢é˜ŸæŠ¥å‘Šã€‚

"å¾ˆå¥½ï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦è®©å…¶ä»–æœåŠ¡èƒ½è®¿é—®åˆ°è¿™ä¸ª Web åº”ç”¨ã€‚" æ¶æ„å¸ˆè¯´é“ï¼Œ"åˆ›å»ºä¸€ä¸ª Service æ¥æš´éœ²å®ƒã€‚"

"ç­‰ç­‰ï¼Œ" ä»–çœ‹äº†ä¸€çœ¼é…ç½®ï¼Œ"è¿™ä¸ª Deployment çš„ Pod ä¼¼ä¹æ²¡æœ‰åˆé€‚çš„æ ‡ç­¾ï¼ŒService éœ€è¦é€šè¿‡ Label Selector æ¥æ‰¾åˆ°åç«¯ Podã€‚ä½ å…ˆç¡®è®¤ä¸€ä¸‹æ ‡ç­¾é…ç½®ã€‚"

ğŸ’¡ æç¤ºï¼šService é€šè¿‡ selector åŒ¹é… Pod çš„ labels æ¥ç¡®å®šæµé‡è½¬å‘ç›®æ ‡ã€‚`,
    difficulty: 'medium',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { 
                    name: 'webapp', 
                    namespace: 'default',
                    labels: { app: 'webapp' }
                },
                spec: { 
                    replicas: 2, 
                    selector: { matchLabels: { app: 'webapp' } },
                    template: {
                        metadata: { labels: { app: 'webapp' } },
                        spec: { containers: [{ name: 'nginx', image: 'nginx:1.20' }] }
                    }
                },
                status: { replicas: 2, readyReplicas: 2, availableReplicas: 2 }
            }
        ],
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { 
                    name: 'webapp-abc123', 
                    namespace: 'default',
                    labels: { app: 'webapp' },
                    uid: 'pod-webapp-1',
                    creationTimestamp: new Date().toISOString()
                },
                spec: { 
                    containers: [{ name: 'nginx', image: 'nginx:1.20' }],
                    nodeName: 'node01'
                },
                status: { phase: 'Running', podIP: '10.244.1.10', hostIP: '192.168.1.3' }
            },
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { 
                    name: 'webapp-def456', 
                    namespace: 'default',
                    labels: { app: 'webapp' },
                    uid: 'pod-webapp-2',
                    creationTimestamp: new Date().toISOString()
                },
                spec: { 
                    containers: [{ name: 'nginx', image: 'nginx:1.20' }],
                    nodeName: 'node02'
                },
                status: { phase: 'Running', podIP: '10.244.2.11', hostIP: '192.168.1.4' }
            }
        ]
    },
    objectives: [
        {
            id: 'check-labels',
            description: 'æŸ¥çœ‹ Pod çš„æ ‡ç­¾é…ç½®',
            hint: 'ä½¿ç”¨ kubectl get pods --show-labels æˆ– kubectl describe pod',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(get\s+pods?\s+.*--show-labels|describe\s+pod|get\s+pods?\s+-o\s+(yaml|json))/.test(cmd)
            )
        },
        {
            id: 'create-service',
            description: 'åˆ›å»ºä¸€ä¸ª Serviceï¼Œä½¿ç”¨æ­£ç¡®çš„ selector å…³è”åˆ° webapp Pod',
            hint: 'Service çš„ selector éœ€è¦åŒ¹é… Pod çš„ labels',
            checkCondition: (state) => state.services.some(s => 
                s.metadata.name !== 'kubernetes' && 
                s.spec.selector && 
                (s.spec.selector['app'] === 'webapp' || Object.keys(s.spec.selector).length > 0)
            )
        },
        {
            id: 'verify-service',
            description: 'éªŒè¯ Service åˆ›å»ºæˆåŠŸå¹¶æŸ¥çœ‹è¯¦æƒ…',
            hint: 'ä½¿ç”¨ kubectl get svc å’Œ kubectl describe svc',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+(get|describe)\s+svc/.test(cmd))
        }
    ],
    hints: [
        'å…ˆç”¨ kubectl get pods --show-labels æŸ¥çœ‹ Pod çš„æ ‡ç­¾',
        'kubectl expose deployment webapp --name=webapp-svc --port=80 --target-port=80 --type=ClusterIP',
        'å‘½ä»¤æ ¼å¼: kubectl expose <èµ„æºç±»å‹> <èµ„æºå> --name=<æœåŠ¡å> --port=<æœåŠ¡ç«¯å£> --target-port=<å®¹å™¨ç«¯å£>',
        'Service çš„ selector ä¼šè‡ªåŠ¨ä» Deployment ç»§æ‰¿ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š'
    ],
    rewards: { xp: 100 }
};

// ========== ç¬¬ä¸‰ç« ï¼šé…ç½®ç®¡ç† ==========

const scenario3_1: Scenario = {
    id: '3-1',
    title: 'ConfigMap æŒ‚è½½å®æˆ˜',
    description: 'åˆ›å»º ConfigMap å¹¶æŒ‚è½½åˆ° Pod ä¸­ä½¿ç”¨',
    story: `"æˆ‘ä»¬çš„åº”ç”¨éœ€è¦è¯»å–æ•°æ®åº“è¿æ¥é…ç½®ï¼Œ" å¼€å‘åŒäº‹è¯´ï¼Œ"ä½†æˆ‘ä¸æƒ³æŠŠé…ç½®ç¡¬ç¼–ç åœ¨é•œåƒé‡Œã€‚"

"æ²¡é—®é¢˜ï¼Œ" ä½ å›ç­”ï¼Œ"Kubernetes çš„ ConfigMap å¯ä»¥å°†é…ç½®ä¸é•œåƒè§£è€¦ã€‚ä½ å¯ä»¥æŠŠé…ç½®å­˜åœ¨ ConfigMap é‡Œï¼Œç„¶åæŒ‚è½½åˆ° Pod ä¸­ã€‚"

"æŒ‚è½½ï¼Ÿæ€ä¹ˆæŒ‚è½½ï¼Ÿ" å¼€å‘åŒäº‹ä¸€è„¸æ‡µã€‚

"æœ‰ä¸¤ç§æ–¹å¼ï¼šä½œä¸ºç¯å¢ƒå˜é‡æ³¨å…¥ï¼Œæˆ–è€…ä½œä¸ºæ–‡ä»¶æŒ‚è½½åˆ°å®¹å™¨å†…ã€‚è®©æˆ‘æ¼”ç¤ºä¸€ä¸‹ã€‚"

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ "Configure a Pod to Use a ConfigMap" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    initialState: {
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { 
                    name: 'demo-app', 
                    namespace: 'default',
                    labels: { app: 'demo' }
                },
                spec: { 
                    containers: [{ name: 'app', image: 'busybox', command: ['sleep', '3600'] }],
                    nodeName: 'node01'
                },
                status: { phase: 'Running', podIP: '10.244.1.20', hostIP: '192.168.1.3' }
            }
        ]
    },
    objectives: [
        {
            id: 'create-configmap',
            description: 'åˆ›å»ºä¸€ä¸ª ConfigMapï¼ŒåŒ…å«è‡³å°‘ä¸€ä¸ªé…ç½®é¡¹ï¼ˆå¦‚ DB_HOST=mysql.localï¼‰',
            hint: 'ä½¿ç”¨ kubectl create configmap æˆ–ç¼–å†™ YAML',
            checkCondition: (state) => state.configMaps.length > 0
        },
        {
            id: 'view-configmap',
            description: 'æŸ¥çœ‹ ConfigMap çš„å†…å®¹ï¼Œç¡®è®¤é…ç½®æ­£ç¡®',
            hint: 'ä½¿ç”¨ kubectl describe configmap æˆ– kubectl get configmap -o yaml',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(describe\s+configmap|get\s+configmaps?\s+.*-o\s+(yaml|json))/.test(cmd)
            )
        },
        {
            id: 'create-pod-with-configmap',
            description: 'åˆ›å»ºæ–° Podï¼Œå°† ConfigMap ä½œä¸ºç¯å¢ƒå˜é‡æˆ– Volume æŒ‚è½½',
            hint: 'åœ¨ Pod YAML ä¸­ä½¿ç”¨ envFrom æˆ– volumes + volumeMounts',
            checkCondition: (state) => state.pods.some(p => 
                // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† envFrom å¼•ç”¨ ConfigMap
                p.spec.containers.some(c => 
                    c.envFrom?.some(e => e.configMapRef) ||
                    c.env?.some(e => e.valueFrom?.configMapKeyRef)
                ) ||
                // æˆ–è€…æ£€æŸ¥æ˜¯å¦æŒ‚è½½äº† ConfigMap volume
                (p.spec.volumes?.some(v => v.configMap) &&
                 p.spec.containers.some(c => (c.volumeMounts?.length ?? 0) > 0))
            )
        },
        {
            id: 'verify-config',
            description: 'è¿›å…¥ Pod éªŒè¯é…ç½®å·²ç”Ÿæ•ˆï¼ˆä½¿ç”¨ exec æŸ¥çœ‹ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶ï¼‰',
            hint: 'ä½¿ç”¨ kubectl exec <pod> -- env æˆ– cat /path/to/config',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+exec\s+.*(--)?\s*(env|cat|printenv|echo)/.test(cmd)
            )
        }
    ],
    hints: [
        'åˆ›å»º ConfigMap: kubectl create configmap myconfig --from-literal=DB_HOST=mysql.local',
        'æ–¹å¼ä¸€ - ç¯å¢ƒå˜é‡: spec.containers[].envFrom[].configMapRef.name',
        'æ–¹å¼äºŒ - Volume: spec.volumes[].configMap.name + spec.containers[].volumeMounts',
        'éªŒè¯: kubectl exec <pod> -- env | grep DB_HOST'
    ],
    rewards: { xp: 120 }
};

const scenario3_2: Scenario = {
    id: '3-2',
    title: 'Secret æ•æ„Ÿæ•°æ®ç®¡ç†',
    description: 'ä½¿ç”¨ Secret å®‰å…¨åœ°å­˜å‚¨å’Œä½¿ç”¨æ•æ„Ÿä¿¡æ¯',
    story: `"æ•°æ®åº“å¯†ç ä¸èƒ½æ˜æ–‡å­˜å‚¨ï¼" å®‰å…¨ä¸»ç®¡ä¸¥è‚ƒåœ°è¯´ã€‚

"ConfigMap æ˜¯æ˜æ–‡çš„ï¼Œä»»ä½•æœ‰æƒé™çš„äººéƒ½èƒ½çœ‹åˆ°ã€‚å¯†ç ã€API Key è¿™ç±»æ•æ„Ÿä¿¡æ¯å¿…é¡»ç”¨ Secretã€‚"

ä½ éœ€è¦åˆ›å»ºä¸€ä¸ª Secret å­˜å‚¨æ•°æ®åº“å¯†ç ï¼Œå¹¶å®‰å…¨åœ°æŒ‚è½½åˆ°åº”ç”¨ Pod ä¸­ã€‚

ğŸ’¡ æç¤ºï¼šSecret çš„å€¼éœ€è¦ base64 ç¼–ç ï¼Œæˆ–ä½¿ç”¨ --from-literal è‡ªåŠ¨ç¼–ç ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-secret',
            description: 'åˆ›å»ºä¸€ä¸ª Secretï¼Œå­˜å‚¨æ•°æ®åº“ç”¨æˆ·åå’Œå¯†ç ',
            hint: 'ä½¿ç”¨ kubectl create secret generic',
            checkCondition: (state) => state.secrets.length > 0
        },
        {
            id: 'verify-secret-encoded',
            description: 'æŸ¥çœ‹ Secret å†…å®¹ï¼Œç†è§£ base64 ç¼–ç ',
            hint: 'ä½¿ç”¨ kubectl get secret <name> -o yaml æŸ¥çœ‹ç¼–ç åçš„å€¼',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+secrets?\s+.*-o\s+(yaml|json)/.test(cmd)
            )
        },
        {
            id: 'mount-secret-to-pod',
            description: 'åˆ›å»º Podï¼Œå°† Secret ä½œä¸ºç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶æŒ‚è½½',
            hint: 'Secret çš„æŒ‚è½½æ–¹å¼ä¸ ConfigMap ç±»ä¼¼',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.containers.some(c => 
                    c.envFrom?.some(e => e.secretRef) ||
                    c.env?.some(e => e.valueFrom?.secretKeyRef)
                ) ||
                (p.spec.volumes?.some(v => v.secret) &&
                 p.spec.containers.some(c => (c.volumeMounts?.length ?? 0) > 0))
            )
        }
    ],
    hints: [
        'åˆ›å»º: kubectl create secret generic db-creds --from-literal=username=admin --from-literal=password=secret123',
        'æŸ¥çœ‹: kubectl get secret db-creds -o yaml (æ³¨æ„ data æ˜¯ base64 ç¼–ç çš„)',
        'è§£ç : echo "c2VjcmV0MTIz" | base64 -d',
        'æŒ‚è½½ä¸ºç¯å¢ƒå˜é‡: env[].valueFrom.secretKeyRef'
    ],
    rewards: { xp: 130 }
};

// ========== ç¬¬å››ç« ï¼šæ•…éšœæ’æŸ¥ ==========

const scenario4_1: Scenario = {
    id: '4-1',
    title: 'ç´§æ€¥æ•…éšœ',
    description: 'æ’æŸ¥å¹¶ä¿®å¤ Deployment ç®¡ç†çš„ Pod æ•…éšœ',
    story: `å‡Œæ™¨ 3 ç‚¹ï¼Œä½ è¢«ç”µè¯å«é†’ã€‚

"ç´§æ€¥æƒ…å†µï¼ç”Ÿäº§ç¯å¢ƒçš„ critical-app æœåŠ¡å…¨æŒ‚äº†ï¼" å€¼ç­åŒäº‹ç„¦æ€¥åœ°è¯´ã€‚

ä½ è¿…é€Ÿæ‰“å¼€ç”µè„‘ï¼Œå‘ç° Deployment ç®¡ç†çš„æ‰€æœ‰ Pod éƒ½åœ¨ CrashLoopBackOffã€‚
çœ‹èµ·æ¥æ˜¯æœ‰äººè¯¯æ”¹äº† Deployment é…ç½®å¯¼è‡´åº”ç”¨æ— æ³•å¯åŠ¨...

ä½œä¸ºæœ‰ç»éªŒçš„ SREï¼Œä½ çŸ¥é“åº”è¯¥ä¿®æ”¹ Deployment é…ç½®æ¥è§¦å‘æ»šåŠ¨æ›´æ–°ï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨åˆ é™¤ Podã€‚`,
    difficulty: 'hard',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { 
                    name: 'critical-app', 
                    namespace: 'default',
                    labels: { app: 'critical' }
                },
                spec: {
                    replicas: 2,
                    selector: { matchLabels: { app: 'critical' } },
                    template: {
                        metadata: { labels: { app: 'critical' } },
                        spec: {
                            containers: [{
                                name: 'app',
                                image: 'myapp:v1',
                                env: [
                                    { name: 'DB_HOST', value: '' },  // ç©ºå€¼å¯¼è‡´å´©æºƒ
                                    { name: 'DB_PORT', value: '3306' }
                                ]
                            }]
                        }
                    }
                },
                status: { replicas: 2, readyReplicas: 0, availableReplicas: 0 }
            }
        ],
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { 
                    name: 'critical-app-7d4b8c6f5-x2k9m', 
                    namespace: 'default', 
                    labels: { app: 'critical', 'pod-template-hash': '7d4b8c6f5' }
                },
                spec: { 
                    containers: [{
                        name: 'app',
                        image: 'myapp:v1',
                        env: [
                            { name: 'DB_HOST', value: '' },
                            { name: 'DB_PORT', value: '3306' }
                        ]
                    }],
                    restartPolicy: 'Always',
                    nodeName: 'node01'
                },
                status: { 
                    phase: 'CrashLoopBackOff',
                    podIP: '10.244.1.100',
                    containerStatuses: [{
                        name: 'app',
                        ready: false,
                        restartCount: 5,
                        state: { waiting: { reason: 'CrashLoopBackOff' } },
                        image: 'myapp:v1',
                        imageID: 'docker://sha256:abc123'
                    }]
                }
            },
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { 
                    name: 'critical-app-7d4b8c6f5-p8n3j', 
                    namespace: 'default', 
                    labels: { app: 'critical', 'pod-template-hash': '7d4b8c6f5' }
                },
                spec: { 
                    containers: [{
                        name: 'app',
                        image: 'myapp:v1',
                        env: [
                            { name: 'DB_HOST', value: '' },
                            { name: 'DB_PORT', value: '3306' }
                        ]
                    }],
                    restartPolicy: 'Always',
                    nodeName: 'node02'
                },
                status: { 
                    phase: 'CrashLoopBackOff',
                    podIP: '10.244.2.100',
                    containerStatuses: [{
                        name: 'app',
                        ready: false,
                        restartCount: 5,
                        state: { waiting: { reason: 'CrashLoopBackOff' } },
                        image: 'myapp:v1',
                        imageID: 'docker://sha256:abc123'
                    }]
                }
            }
        ],
        configMaps: [
            {
                apiVersion: 'v1',
                kind: 'ConfigMap',
                metadata: { name: 'app-config', namespace: 'default' },
                data: { DB_HOST: 'mysql.default.svc.cluster.local', DB_PORT: '3306' }
            }
        ]
    },
    objectives: [
        {
            id: 'check-deployment',
            description: 'æŸ¥çœ‹ Deployment å’Œ Pod çŠ¶æ€',
            hint: 'è¿è¡Œï¼škubectl get deploy,pods æŸ¥çœ‹èµ„æºçŠ¶æ€',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(deploy|deployment|pods|pod|all)/.test(cmd)
            )
        },
        {
            id: 'check-logs',
            description: 'ä½¿ç”¨ kubectl logs æŸ¥çœ‹å®¹å™¨æ—¥å¿—æ‰¾å‡ºå´©æºƒåŸå› ',
            hint: 'è¿è¡Œï¼škubectl logs <podåç§°>ï¼Œæ³¨æ„ ERROR ä¿¡æ¯',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+logs\s+critical-app/.test(cmd)
            )
        },
        {
            id: 'fix-deployment',
            description: 'ä¿®æ”¹ Deployment é…ç½®è§¦å‘æ»šåŠ¨æ›´æ–°',
            hint: 'ä½¿ç”¨ kubectl edit deploy critical-app æˆ– kubectl set env ä¿®å¤ DB_HOST',
            checkCondition: (state) => {
                // æ£€æŸ¥ Deployment çš„ Pod æ˜¯å¦æœ‰æ­£ç¡®çš„ DB_HOST é…ç½®
                const deploy = state.deployments.find(d => d.metadata.name === 'critical-app');
                if (!deploy) return false;
                const env = deploy.spec.template.spec.containers[0]?.env;
                const dbHost = env?.find(e => e.name === 'DB_HOST');
                return dbHost?.value !== undefined && dbHost.value.length > 0;
            }
        }
    ],
    hints: [
        'ç¬¬ä¸€æ­¥ï¼škubectl get deploy,pods æŸ¥çœ‹ Deployment å’Œ Pod çŠ¶æ€',
        'ç¬¬äºŒæ­¥ï¼škubectl logs <podåç§°> æŸ¥çœ‹é”™è¯¯æ—¥å¿—',
        'æ—¥å¿—æ˜¾ç¤º DB_HOST ç¯å¢ƒå˜é‡ä¸ºç©ºå¯¼è‡´å´©æºƒ',
        'ç¬¬ä¸‰æ­¥ï¼škubectl edit deploy critical-app ç¼–è¾‘ Deployment',
        'æ‰¾åˆ° env éƒ¨åˆ†ï¼Œå°† DB_HOST çš„ value ä»ç©ºæ”¹ä¸º mysql.default.svc.cluster.local',
        'ä¿å­˜å Deployment ä¼šè‡ªåŠ¨è§¦å‘æ»šåŠ¨æ›´æ–°ï¼Œåˆ›å»ºæ–°çš„å¥åº· Pod',
        'ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼škubectl set env deploy/critical-app DB_HOST=mysql.default.svc.cluster.local'
    ],
    rewards: { xp: 150, badges: ['first-responder'] }
};

// ========== ç¬¬äº”ç« ï¼šRBAC å®‰å…¨ ==========

const scenario5_1: Scenario = {
    id: '5-1',
    title: 'æƒé™ç®¡ç†',
    description: 'é…ç½® RBAC æƒé™æ§åˆ¶',
    story: `"å®‰å…¨å›¢é˜Ÿè¦æ±‚æˆ‘ä»¬åŠ å¼ºæƒé™ç®¡æ§ï¼Œ" å®‰å…¨ä¸»ç®¡è¯´é“ã€‚

"æˆ‘ä»¬éœ€è¦ä¸ºå¼€å‘å›¢é˜Ÿåˆ›å»ºä¸€ä¸ªåªè¯»è´¦å·ï¼Œè®©ä»–ä»¬åªèƒ½æŸ¥çœ‹ Pod çŠ¶æ€ï¼Œä¸èƒ½ä¿®æ”¹ã€‚
è¯·å…ˆåˆ›å»ºä¸€ä¸ª ServiceAccountï¼Œç„¶åé…ç½® Role å’Œ RoleBindingã€‚"`,
    difficulty: 'hard',
    objectives: [
        {
            id: 'create-sa',
            description: 'åˆ›å»ºä¸€ä¸ªåä¸º pod-reader çš„ ServiceAccount',
            hint: 'ä½¿ç”¨ kubectl create sa pod-reader',
            checkCondition: (state) => state.serviceAccounts.some(sa => sa.metadata.name === 'pod-reader')
        },
        {
            id: 'create-role',
            description: 'åˆ›å»ºä¸€ä¸ªåä¸º pod-reader çš„ Roleï¼Œåªå…è®¸ getã€listã€watch pods',
            hint: 'ä½¿ç”¨ kubectl create role pod-reader --verb=get,list,watch --resource=pods',
            checkCondition: (state) => state.roles.some(r => 
                r.metadata.name === 'pod-reader' && 
                r.rules.some(rule => 
                    rule.resources.includes('pods') && 
                    rule.verbs.some(v => ['get', 'list', 'watch'].includes(v))
                )
            )
        },
        {
            id: 'create-rolebinding',
            description: 'åˆ›å»º RoleBinding å°† pod-reader Role ç»‘å®šç»™ pod-reader ServiceAccount',
            hint: 'ä½¿ç”¨ kubectl create rolebinding pod-reader-binding --role=pod-reader --serviceaccount=default:pod-reader',
            checkCondition: (state) => state.roleBindings.some(rb => 
                rb.roleRef.name === 'pod-reader' && 
                rb.subjects.some(s => s.kind === 'ServiceAccount' && s.name === 'pod-reader')
            )
        },
        {
            id: 'verify-permissions',
            description: 'éªŒè¯æƒé™è®¾ç½®æ­£ç¡®',
            hint: 'ä½¿ç”¨ kubectl auth can-i get pods --as=system:serviceaccount:default:pod-reader',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+auth\s+can-i/.test(cmd))
        }
    ],
    hints: [
        '1. kubectl create sa pod-reader - åˆ›å»º ServiceAccount',
        '2. kubectl create role pod-reader --verb=get,list,watch --resource=pods',
        '3. kubectl create rolebinding pod-reader-binding --role=pod-reader --serviceaccount=default:pod-reader',
        '4. kubectl auth can-i get pods --as=system:serviceaccount:default:pod-reader'
    ],
    rewards: { xp: 150, badges: ['security-master'] }
};

// ========== ç¬¬å…­ç« ï¼šETCD ç®¡ç† ==========

const scenario6_1: Scenario = {
    id: '6-1',
    title: 'ETCD å¤‡ä»½',
    description: 'å­¦ä¹  ETCD å¤‡ä»½æ“ä½œï¼ˆéœ€è¦æ­£ç¡®æŒ‡å®šè¯ä¹¦ï¼‰',
    story: `"ç”Ÿäº§ç¯å¢ƒçš„ ETCD å¿…é¡»å®šæœŸå¤‡ä»½ï¼Œ" è¿ç»´ä¸»ç®¡ä¸¥è‚ƒåœ°è¯´ã€‚

"æ³¨æ„ï¼šETCD ä½¿ç”¨ TLS åŠ å¯†é€šä¿¡ï¼Œæ‰§è¡Œä»»ä½•å‘½ä»¤éƒ½éœ€è¦æŒ‡å®šæ­£ç¡®çš„è¯ä¹¦è·¯å¾„å’Œç«¯ç‚¹åœ°å€ã€‚
è¿™æ˜¯ CKA è€ƒè¯•çš„å¿…è€ƒå†…å®¹ï¼ŒåŠ¡å¿…ç†Ÿç»ƒæŒæ¡ï¼"

ğŸ“ ETCD é…ç½®ä¿¡æ¯ï¼š
- ç«¯ç‚¹ï¼šhttps://127.0.0.1:2379
- CA è¯ä¹¦ï¼š/etc/kubernetes/pki/etcd/ca.crt
- å®¢æˆ·ç«¯è¯ä¹¦ï¼š/etc/kubernetes/pki/etcd/server.crt
- å®¢æˆ·ç«¯å¯†é’¥ï¼š/etc/kubernetes/pki/etcd/server.key`,
    difficulty: 'hard',
    objectives: [
        {
            id: 'check-etcd-health',
            description: 'ä½¿ç”¨æ­£ç¡®çš„è¯ä¹¦å‚æ•°æ£€æŸ¥ ETCD å¥åº·çŠ¶æ€',
            hint: 'etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key endpoint health',
            checkCondition: (_state, history) => history.some(cmd => 
                /etcdctl/.test(cmd) && 
                /--cacert/.test(cmd) && 
                /--cert[^a]/.test(cmd) && 
                /--key/.test(cmd) && 
                /endpoint\s+health/.test(cmd)
            )
        },
        {
            id: 'create-backup',
            description: 'åˆ›å»º ETCD å¿«ç…§å¤‡ä»½åˆ° /data/etcd-backup/snapshot.db',
            hint: 'etcdctl --endpoints=... --cacert=... --cert=... --key=... snapshot save /data/etcd-backup/snapshot.db',
            checkCondition: (state) => state.etcd.backups.some(b => b.path.includes('snapshot'))
        },
        {
            id: 'verify-backup',
            description: 'éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§',
            hint: 'etcdctl snapshot status /data/etcd-backup/snapshot.db',
            checkCondition: (_state, history) => history.some(cmd => /etcdctl\s+snapshot\s+status/.test(cmd))
        }
    ],
    hints: [
        'ğŸ“‹ å®Œæ•´å¤‡ä»½å‘½ä»¤ç¤ºä¾‹ï¼š',
        'etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /data/etcd-backup/snapshot.db',
        '',
        'ğŸ’¡ éªŒè¯å¤‡ä»½ï¼ˆä¸éœ€è¦è¯ä¹¦ï¼‰ï¼š',
        'etcdctl snapshot status /data/etcd-backup/snapshot.db --write-out=table'
    ],
    rewards: { xp: 150, badges: ['etcd-backup'] }
};

const scenario6_2: Scenario = {
    id: '6-2',
    title: 'ETCD æ¢å¤',
    description: 'ä»å¤‡ä»½æ¢å¤æŸåçš„ ETCD æ•°æ®',
    story: `ğŸš¨ ç´§æ€¥å‘Šè­¦ï¼šETCD æ•°æ®æŸåï¼

"ç³Ÿç³•ï¼ETCD æ•°æ®ç›®å½•è¢«è¯¯åˆ äº†ï¼" å€¼ç­å·¥ç¨‹å¸ˆæ»¡å¤´å¤§æ±—ã€‚
"æ‰€æœ‰ kubectl å‘½ä»¤éƒ½ä¸èƒ½ç”¨äº†ï¼é›†ç¾¤å®Œå…¨ç˜«ç—ªï¼"

å¥½æ¶ˆæ¯æ˜¯ï¼šæˆ‘ä»¬æœ‰æ˜¨å¤©çš„å¤‡ä»½æ–‡ä»¶ /data/etcd-backup/snapshot.db

ä½ éœ€è¦ï¼š
1. åœæ­¢ etcd æœåŠ¡
2. åˆ é™¤æŸåçš„æ•°æ®ç›®å½•
3. ä»å¤‡ä»½æ¢å¤æ•°æ®
4. é‡å¯æœåŠ¡éªŒè¯æ¢å¤æˆåŠŸ`,
    difficulty: 'hard',
    initialState: {
        etcd: {
            members: [{
                id: '8e9e05c52164694d',
                name: 'master',
                peerURLs: ['https://192.168.1.2:2380'],
                clientURLs: ['https://127.0.0.1:2379'],
                status: 'unhealthy',
                isLeader: false,
                dbSize: 0,
                dbSizeInUse: 0
            }],
            version: '3.5.9',
            clusterID: 'cdf818194e3a8c32',
            backups: [{
                name: 'snapshot.db',
                timestamp: new Date(Date.now() - 86400000).toISOString(),
                size: 4194304,
                path: '/data/etcd-backup/snapshot.db'
            }],
            corrupted: true
        }
    },
    objectives: [
        {
            id: 'verify-corruption',
            description: 'ç¡®è®¤ ETCD æ•°æ®å·²æŸåï¼ˆkubectl å‘½ä»¤ä¸å¯ç”¨ï¼‰',
            hint: 'å°è¯•è¿è¡Œ kubectl get pods è§‚å¯Ÿé”™è¯¯',
            checkCondition: (_state, history) => history.some(cmd => /kubectl/.test(cmd))
        },
        {
            id: 'restore-snapshot',
            description: 'ä½¿ç”¨å¤‡ä»½æ–‡ä»¶æ¢å¤ ETCD æ•°æ®',
            hint: 'etcdctl snapshot restore /data/etcd-backup/snapshot.db --data-dir=/var/lib/etcd-restored',
            checkCondition: (state) => state.etcd.corrupted === false
        },
        {
            id: 'verify-recovery',
            description: 'éªŒè¯ ETCD å·²æ¢å¤æ­£å¸¸',
            hint: 'å†æ¬¡è¿è¡Œ etcdctl endpoint health æ£€æŸ¥çŠ¶æ€',
            checkCondition: (state, history) => 
                !state.etcd.corrupted && 
                history.some(cmd => /etcdctl.*endpoint\s+health/.test(cmd))
        }
    ],
    hints: [
        'âš ï¸ ETCD æŸåæ—¶ï¼Œkubectl å‘½ä»¤ä¼šæŠ¥é”™ï¼šæ— æ³•è¿æ¥åˆ° API Server',
        '',
        'ğŸ“‹ æ¢å¤å‘½ä»¤ï¼ˆä¸éœ€è¦è¯ä¹¦ï¼Œæ“ä½œæœ¬åœ°æ–‡ä»¶ï¼‰ï¼š',
        'etcdctl snapshot restore /data/etcd-backup/snapshot.db --data-dir=/var/lib/etcd',
        '',
        'ğŸ“‹ éªŒè¯æ¢å¤ï¼ˆéœ€è¦è¯ä¹¦ï¼‰ï¼š',
        'etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key endpoint health'
    ],
    rewards: { xp: 200, badges: ['etcd-recovery', 'disaster-recovery'] }
};

// ========== ç¬¬ä¸ƒç« ï¼šèµ„æºç®¡ç† ==========

const scenario7_1: Scenario = {
    id: '7-1',
    title: 'Pod èµ„æºé™åˆ¶',
    description: 'ä¸º Pod é…ç½® CPU å’Œå†…å­˜é™åˆ¶',
    story: `è¿ç»´ä¸»ç®¡æ‰¾åˆ°ä½ ï¼š"æœ€è¿‘æœ‰ä¸ªåº”ç”¨æŠŠæ•´ä¸ªèŠ‚ç‚¹çš„å†…å­˜éƒ½åƒå…‰äº†ï¼Œå¯¼è‡´å…¶ä»–æœåŠ¡å…¨æŒ‚äº†ã€‚"

"æˆ‘ä»¬éœ€è¦ç»™æ‰€æœ‰ Pod è®¾ç½®èµ„æºé™åˆ¶ï¼Œé˜²æ­¢å•ä¸ªåº”ç”¨å½±å“æ•´ä¸ªé›†ç¾¤ã€‚ä½ å…ˆåˆ›å»ºä¸€ä¸ªå¸¦èµ„æºé™åˆ¶çš„ Pod è¯•è¯•ã€‚"

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes å®˜æ–¹æ–‡æ¡£ä¸­å…³äº Resource Management çš„ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-pod-with-limits',
            description: 'åˆ›å»ºä¸€ä¸ª Podï¼Œè®¾ç½® CPU å’Œå†…å­˜çš„ requests ä¸ limits',
            hint: 'åœ¨ Pod spec çš„ containers ä¸­æ·»åŠ  resources å­—æ®µ',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.containers.some(c => 
                    c.resources?.limits?.cpu && c.resources?.limits?.memory &&
                    c.resources?.requests?.cpu && c.resources?.requests?.memory
                )
            )
        },
        {
            id: 'verify-resources',
            description: 'ä½¿ç”¨ describe å‘½ä»¤éªŒè¯èµ„æºé…ç½®å·²ç”Ÿæ•ˆ',
            hint: 'æŸ¥çœ‹ Pod è¯¦æƒ…ä¸­çš„ Limits å’Œ Requests éƒ¨åˆ†',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+describe\s+pod/.test(cmd))
        }
    ],
    hints: [
        'èµ„æºé…ç½®ä½äº spec.containers[].resources',
        'requests: è°ƒåº¦æ—¶çš„æœ€å°ä¿è¯èµ„æº',
        'limits: å®¹å™¨èƒ½ä½¿ç”¨çš„æœ€å¤§èµ„æº',
        'æ ¼å¼ç¤ºä¾‹: cpu: "100m", memory: "128Mi"'
    ],
    rewards: { xp: 120 }
};

const scenario7_2: Scenario = {
    id: '7-2',
    title: 'èµ„æºé™åˆ¶æ•…éšœæ’æŸ¥',
    description: 'è¯Šæ–­å¹¶ä¿®å¤å› èµ„æºé…ç½®ä¸å½“å¯¼è‡´çš„ Pod è°ƒåº¦å¤±è´¥é—®é¢˜',
    story: `ğŸš¨ å‘Šè­¦ï¼šç”Ÿäº§ç¯å¢ƒæœ‰ Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€ï¼

å¼€å‘å›¢é˜Ÿç´§æ€¥ä¸Šçº¿äº†ä¸€ä¸ªæ–°æœåŠ¡ memory-hungryï¼Œä½† Pod å·²ç» Pending äº† 30 åˆ†é’Ÿã€‚

"æˆ‘æ˜æ˜è®¾ç½®äº†èµ„æºé™åˆ¶å•Šï¼Œ" å¼€å‘è€…å¾ˆå›°æƒ‘ï¼Œ"ä¸ºä»€ä¹ˆè°ƒåº¦ä¸ä¸Šå»ï¼Ÿ"

ä½ éœ€è¦ï¼š
1. è¯Šæ–­ Pod ä¸ºä»€ä¹ˆæ— æ³•è¢«è°ƒåº¦
2. æ£€æŸ¥èŠ‚ç‚¹çš„å¯ç”¨èµ„æº
3. è°ƒæ•´ Pod çš„èµ„æºè¯·æ±‚ï¼Œä½¿å…¶èƒ½å¤Ÿè¢«æ­£å¸¸è°ƒåº¦`,
    difficulty: 'medium',
    initialState: {
        pods: [{
            apiVersion: 'v1',
            kind: 'Pod',
            metadata: {
                name: 'memory-hungry',
                namespace: 'default',
                labels: { app: 'memory-hungry' },
                uid: 'pending-pod-uid',
                creationTimestamp: new Date(Date.now() - 1800000).toISOString()
            },
            spec: {
                containers: [{
                    name: 'app',
                    image: 'nginx:1.20',
                    resources: {
                        requests: { cpu: '4000m', memory: '16Gi' },
                        limits: { cpu: '8000m', memory: '32Gi' }
                    }
                }],
                nodeName: ''
            },
            status: {
                phase: 'Pending',
                conditions: [{
                    type: 'PodScheduled',
                    status: 'False',
                    reason: 'Unschedulable',
                    message: '0/3 nodes are available: 1 node(s) had untolerable taint, 2 Insufficient memory.'
                }]
            }
        }]
    },
    objectives: [
        {
            id: 'diagnose-pending',
            description: 'ä½¿ç”¨ describe å‘½ä»¤è¯Šæ–­ Pod æ— æ³•è°ƒåº¦çš„åŸå› ',
            hint: 'kubectl describe pod memory-hungry',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+describe\s+pod\s+memory-hungry/.test(cmd)
            )
        },
        {
            id: 'check-node-resources',
            description: 'æ£€æŸ¥èŠ‚ç‚¹çš„å¯åˆ†é…èµ„æº',
            hint: 'kubectl describe node æˆ– kubectl top nodes',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(describe\s+node|top\s+node)/.test(cmd)
            )
        },
        {
            id: 'fix-resource-requests',
            description: 'åˆ é™¤æ—§ Pod å¹¶åˆ›å»ºèµ„æºè¯·æ±‚åˆç†çš„æ–° Podï¼ˆrequests.memory <= 2Giï¼‰',
            hint: 'å…ˆåˆ é™¤æ—§ Podï¼Œå†åˆ›å»ºä¸€ä¸ª memory requests ä¸è¶…è¿‡ 2Gi çš„ Pod',
            checkCondition: (state) => state.pods.some(p => 
                p.metadata.name === 'memory-hungry' &&
                p.status.phase === 'Running' &&
                p.spec.containers.some(c => {
                    const memStr = c.resources?.requests?.memory || '';
                    const memValue = parseInt(memStr);
                    // æ£€æŸ¥å†…å­˜è¯·æ±‚æ˜¯å¦åˆç†ï¼ˆ<= 2Giï¼‰
                    if (memStr.includes('Gi')) return memValue <= 2;
                    if (memStr.includes('Mi')) return memValue <= 2048;
                    return false;
                })
            )
        }
    ],
    hints: [
        'ğŸ“‹ æ’æŸ¥æ­¥éª¤ï¼š',
        '1. kubectl describe pod memory-hungry  æŸ¥çœ‹ Events ä¸­çš„è°ƒåº¦å¤±è´¥åŸå› ',
        '2. kubectl describe node node01  æŸ¥çœ‹ Allocatable èµ„æº',
        '3. kubectl delete pod memory-hungry  åˆ é™¤æœ‰é—®é¢˜çš„ Pod',
        '4. åˆ›å»ºæ–° Podï¼Œé™ä½ resources.requests.memory åˆ°åˆç†å€¼ï¼ˆå¦‚ 512Miï¼‰',
        '',
        'ğŸ’¡ å¸¸è§åŸå› ï¼š',
        '- Insufficient memory: è¯·æ±‚çš„å†…å­˜è¶…è¿‡èŠ‚ç‚¹å¯åˆ†é…å†…å­˜',
        '- Insufficient cpu: è¯·æ±‚çš„ CPU è¶…è¿‡èŠ‚ç‚¹å¯åˆ†é… CPU'
    ],
    rewards: { xp: 150, badges: ['troubleshooter'] }
};

// ========== ç¬¬å…«ç« ï¼šè°ƒåº¦ç­–ç•¥ ==========

const scenario8_1: Scenario = {
    id: '8-1',
    title: 'NodeSelector èŠ‚ç‚¹é€‰æ‹©',
    description: 'ä½¿ç”¨æ ‡ç­¾é€‰æ‹©å™¨å°† Pod è°ƒåº¦åˆ°æŒ‡å®šèŠ‚ç‚¹',
    story: `å…¬å¸è´­ä¹°äº†ä¸€æ‰¹å¸¦ GPU çš„æœåŠ¡å™¨ç”¨äºæœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚

"æˆ‘ä»¬éœ€è¦ç¡®ä¿ ML è®­ç»ƒä»»åŠ¡åªè¿è¡Œåœ¨ GPU èŠ‚ç‚¹ä¸Šï¼Œ" æ¶æ„å¸ˆè§£é‡Šé“ï¼Œ"æ™®é€šä»»åŠ¡ä¸åº”è¯¥å ç”¨è¿™äº›æ˜‚è´µçš„èµ„æºã€‚"

ä½ çš„ä»»åŠ¡æ˜¯ç»™èŠ‚ç‚¹æ‰“ä¸Šæ ‡ç­¾ï¼Œç„¶åè®© Pod åªè°ƒåº¦åˆ°ç‰¹å®šæ ‡ç­¾çš„èŠ‚ç‚¹ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Assigning Pods to Nodes" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'label-node',
            description: 'ç»™é›†ç¾¤ä¸­çš„ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾',
            hint: 'ä½¿ç”¨ kubectl label å‘½ä»¤ç»™ node æ·»åŠ æ ‡ç­¾',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+label\s+node/.test(cmd))
        },
        {
            id: 'create-pod-nodeselector',
            description: 'åˆ›å»ºä¸€ä¸ª Podï¼Œä½¿ç”¨ nodeSelector æŒ‡å®šè°ƒåº¦åˆ°å¸¦æœ‰è¯¥æ ‡ç­¾çš„èŠ‚ç‚¹',
            hint: 'åœ¨ Pod spec ä¸­æ·»åŠ  nodeSelector å­—æ®µ',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.nodeSelector && Object.keys(p.spec.nodeSelector).length > 0
            )
        },
        {
            id: 'verify-scheduling',
            description: 'éªŒè¯ Pod æ˜¯å¦è¢«è°ƒåº¦åˆ°äº†æ­£ç¡®çš„èŠ‚ç‚¹',
            hint: 'ä½¿ç”¨ kubectl get pods -o wide æŸ¥çœ‹èŠ‚ç‚¹åˆ†é…',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(pods?|po).*-o\s+wide/.test(cmd))
        }
    ],
    hints: [
        'ç»™èŠ‚ç‚¹æ‰“æ ‡ç­¾: kubectl label nodes <node-name> <key>=<value>',
        'nodeSelector åœ¨ spec ä¸‹ä¸ containers åŒçº§',
        'æ ¼å¼: nodeSelector: {key: value}',
        'ä½¿ç”¨ -o wide å¯ä»¥çœ‹åˆ° Pod è¿è¡Œåœ¨å“ªä¸ªèŠ‚ç‚¹'
    ],
    rewards: { xp: 140 }
};

const scenario8_2: Scenario = {
    id: '8-2',
    title: 'Taints ä¸ Tolerations',
    description: 'ä½¿ç”¨æ±¡ç‚¹å’Œå®¹å¿åº¦æ§åˆ¶ Pod è°ƒåº¦',
    story: `ç”Ÿäº§é›†ç¾¤ä¸­ï¼Œæ§åˆ¶å¹³é¢èŠ‚ç‚¹ä¸åº”è¯¥è¿è¡Œæ™®é€šå·¥ä½œè´Ÿè½½ã€‚

"æˆ‘ä»¬éœ€è¦ç¡®ä¿åªæœ‰ç‰¹å®šçš„ç³»ç»Ÿç»„ä»¶æ‰èƒ½è¿è¡Œåœ¨ master èŠ‚ç‚¹ä¸Šï¼Œ" SRE å·¥ç¨‹å¸ˆè¯´ï¼Œ"å…¶ä»– Pod å¿…é¡»è°ƒåº¦åˆ°å·¥ä½œèŠ‚ç‚¹ã€‚"

Taintsï¼ˆæ±¡ç‚¹ï¼‰å¯ä»¥è®©èŠ‚ç‚¹æ’æ–¥ Podï¼Œè€Œ Tolerationsï¼ˆå®¹å¿ï¼‰å¯ä»¥è®© Pod å¿½ç•¥æŸäº›æ±¡ç‚¹ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Taints and Tolerations" ç« èŠ‚ã€‚`,
    difficulty: 'hard',
    objectives: [
        {
            id: 'taint-node',
            description: 'ç»™ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹æ·»åŠ  NoSchedule æ•ˆæœçš„æ±¡ç‚¹',
            hint: 'ä½¿ç”¨ kubectl taint å‘½ä»¤',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+taint\s+node/.test(cmd))
        },
        {
            id: 'create-pod-toleration',
            description: 'åˆ›å»ºä¸€ä¸ªå¸¦æœ‰å¯¹åº” toleration çš„ Podï¼Œä½¿å…¶èƒ½è°ƒåº¦åˆ°æœ‰æ±¡ç‚¹çš„èŠ‚ç‚¹',
            hint: 'åœ¨ Pod spec ä¸­æ·»åŠ  tolerations æ•°ç»„',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.tolerations && p.spec.tolerations.length > 0
            )
        },
        {
            id: 'verify-pod-scheduled',
            description: 'éªŒè¯ Pod æˆåŠŸè¿è¡Œ',
            hint: 'æ£€æŸ¥ Pod çŠ¶æ€æ˜¯å¦ä¸º Running',
            checkCondition: (state, history) => 
                history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd)) &&
                state.pods.some(p => p.status.phase === 'Running')
        }
    ],
    hints: [
        'æ·»åŠ æ±¡ç‚¹: kubectl taint nodes <node> key=value:NoSchedule',
        'ç§»é™¤æ±¡ç‚¹: kubectl taint nodes <node> key=value:NoSchedule-',
        'tolerations æ ¼å¼: [{key, operator, value, effect}]',
        'operator å¯ä»¥æ˜¯ Equal æˆ– Exists'
    ],
    rewards: { xp: 160 }
};

// ========== ç¬¬ä¹ç« ï¼šå­˜å‚¨ç®¡ç† ==========

const scenario9_1: Scenario = {
    id: '9-1',
    title: 'PersistentVolume å®Œæ•´å®æˆ˜',
    description: 'åˆ›å»º PV â†’ ç”³è¯· PVC â†’ æŒ‚è½½åˆ° Pod â†’ éªŒè¯æ•°æ®æŒä¹…åŒ–',
    story: `æ•°æ®åº“å›¢é˜Ÿé‡åˆ°äº†ä¸¥é‡é—®é¢˜ï¼š

"MySQL Pod é‡å¯åæ•°æ®å…¨ä¸¢äº†ï¼" DBA ç„¦æ€¥åœ°è¯´ï¼Œ"è¿™ä¸ª Pod ç”¨çš„æ˜¯ emptyDirï¼Œå®¹å™¨ä¸€åˆ æ•°æ®å°±æ²¡äº†ã€‚"

"æˆ‘ä»¬éœ€è¦æŒä¹…åŒ–å­˜å‚¨ï¼Œ" ä½ è§£é‡Šé“ï¼Œ"Kubernetes çš„å­˜å‚¨ä½“ç³»åˆ†ä¸ºä¸‰å±‚ï¼š
1. **PVï¼ˆPersistentVolumeï¼‰**ï¼šç®¡ç†å‘˜é¢„å…ˆé…ç½®çš„å­˜å‚¨èµ„æº
2. **PVCï¼ˆPersistentVolumeClaimï¼‰**ï¼šç”¨æˆ·ç”³è¯·å­˜å‚¨çš„'è®¢å•'
3. **Pod æŒ‚è½½**ï¼šå°† PVC æŒ‚è½½åˆ°å®¹å™¨çš„æŒ‡å®šè·¯å¾„"

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ "Persistent Volumes" ç« èŠ‚ã€‚`,
    difficulty: 'hard',
    objectives: [
        {
            id: 'create-pv',
            description: 'åˆ›å»º PersistentVolumeï¼ˆ1Giï¼ŒaccessMode: ReadWriteOnceï¼‰',
            hint: 'å®šä¹‰ capacityã€accessModesã€hostPathï¼ˆæˆ–å…¶ä»–å­˜å‚¨åç«¯ï¼‰',
            checkCondition: (state) => state.persistentVolumes.length > 0
        },
        {
            id: 'verify-pv',
            description: 'æŸ¥çœ‹ PV çŠ¶æ€ï¼Œç¡®è®¤ä¸º Available',
            hint: 'kubectl get pv æŸ¥çœ‹ STATUS åˆ—',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+pv/.test(cmd))
        },
        {
            id: 'create-pvc',
            description: 'åˆ›å»º PVC ç”³è¯·å­˜å‚¨ï¼ˆå¤§å°å’Œ accessMode è¦åŒ¹é…ï¼‰',
            hint: 'PVC çš„ requests.storage ä¸èƒ½è¶…è¿‡ PV çš„ capacity',
            checkCondition: (state) => state.persistentVolumeClaims.length > 0
        },
        {
            id: 'verify-binding',
            description: 'ç¡®è®¤ PVC ä¸ PV ç»‘å®šæˆåŠŸï¼ˆçŠ¶æ€å˜ä¸º Boundï¼‰',
            hint: 'kubectl get pvc æŸ¥çœ‹ STATUS å’Œ VOLUME åˆ—',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+pvc/.test(cmd))
        },
        {
            id: 'mount-to-pod',
            description: 'åˆ›å»º Podï¼Œå°† PVC æŒ‚è½½åˆ°å®¹å™¨çš„ /data ç›®å½•',
            hint: 'spec.volumes + spec.containers[].volumeMounts',
            checkCondition: (state) => state.pods.some(p =>
                p.spec.volumes?.some(v => v.persistentVolumeClaim) &&
                p.spec.containers.some(c => (c.volumeMounts?.length ?? 0) > 0)
            )
        },
        {
            id: 'verify-mount',
            description: 'è¿›å…¥ Pod éªŒè¯æŒ‚è½½æˆåŠŸï¼ˆå†™å…¥æ–‡ä»¶æµ‹è¯•ï¼‰',
            hint: 'kubectl exec <pod> -- ls /data æˆ– touch /data/test',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+exec/.test(cmd))
        }
    ],
    hints: [
        'PV YAML: spec.capacity.storage: 1Gi, spec.accessModes: [ReadWriteOnce], spec.hostPath.path: /mnt/data',
        'PVC YAML: spec.accessModes: [ReadWriteOnce], spec.resources.requests.storage: 1Gi',
        'Pod volumes: [{name: data, persistentVolumeClaim: {claimName: <pvc-name>}}]',
        'Pod volumeMounts: [{name: data, mountPath: /data}]'
    ],
    rewards: { xp: 200 }
};

// ========== ç¬¬åç« ï¼šç½‘ç»œç­–ç•¥ ==========

const scenario10_1: Scenario = {
    id: '10-1',
    title: 'NetworkPolicy ç½‘ç»œéš”ç¦»',
    description: 'ä½¿ç”¨ç½‘ç»œç­–ç•¥é™åˆ¶ Pod ä¹‹é—´çš„é€šä¿¡',
    story: `å®‰å…¨å®¡è®¡å‘ç°é›†ç¾¤ä¸­çš„ Pod å¯ä»¥ä»»æ„äº’ç›¸è®¿é—®ï¼Œè¿™æ˜¯ä¸€ä¸ªå®‰å…¨éšæ‚£ã€‚

"æˆ‘ä»¬éœ€è¦å®æ–½é›¶ä¿¡ä»»ç½‘ç»œç­–ç•¥ï¼Œ" å®‰å…¨å›¢é˜Ÿè¦æ±‚ï¼Œ"æ•°æ®åº“åªèƒ½è¢«ç‰¹å®šçš„åº”ç”¨è®¿é—®ï¼Œå…¶ä»– Pod ä¸åº”è¯¥èƒ½è¿æ¥åˆ°å®ƒã€‚"

NetworkPolicy å¯ä»¥æ§åˆ¶ Pod çš„å…¥ç«™ï¼ˆIngressï¼‰å’Œå‡ºç«™ï¼ˆEgressï¼‰æµé‡ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Network Policies" ç« èŠ‚ã€‚`,
    difficulty: 'hard',
    objectives: [
        {
            id: 'create-networkpolicy',
            description: 'åˆ›å»ºä¸€ä¸ª NetworkPolicyï¼Œé™åˆ¶å¯¹ç‰¹å®š Pod çš„è®¿é—®',
            hint: 'ä½¿ç”¨ podSelector é€‰æ‹©ç›®æ ‡ Podï¼Œingress å®šä¹‰å…¥ç«™è§„åˆ™',
            checkCondition: (state) => state.networkPolicies.length > 0
        },
        {
            id: 'verify-policy',
            description: 'æŸ¥çœ‹åˆ›å»ºçš„ NetworkPolicy è¯¦æƒ…',
            hint: 'ä½¿ç”¨ kubectl describe æˆ– get -o yaml æŸ¥çœ‹',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(describe|get.*-o\s+yaml)\s+networkpolic/.test(cmd)
            )
        }
    ],
    hints: [
        'NetworkPolicy é€šè¿‡ podSelector é€‰æ‹©è¢«ä¿æŠ¤çš„ Pod',
        'ingress.from å®šä¹‰å…è®¸è®¿é—®çš„æ¥æº',
        'å¯ä»¥æŒ‰ podSelectorã€namespaceSelector æˆ– ipBlock è¿‡æ»¤',
        'ç©ºçš„ ingress åˆ—è¡¨è¡¨ç¤ºæ‹’ç»æ‰€æœ‰å…¥ç«™æµé‡'
    ],
    rewards: { xp: 170 }
};

// ========== ç¬¬åä¸€ç« ï¼šå¤šå®¹å™¨ Pod ==========

const scenario11_1: Scenario = {
    id: '11-1',
    title: 'Sidecar å®¹å™¨æ¨¡å¼',
    description: 'ä½¿ç”¨å¤šå®¹å™¨ Pod å®ç° Sidecar æ¨¡å¼',
    story: `åº”ç”¨å›¢é˜Ÿå¸Œæœ›åœ¨ä¸ä¿®æ”¹åº”ç”¨ä»£ç çš„æƒ…å†µä¸‹æ”¶é›†æ—¥å¿—ã€‚

"æˆ‘ä»¬æƒ³ç”¨ Sidecar å®¹å™¨æ¥å¤„ç†æ—¥å¿—æ”¶é›†ï¼Œ" å¼€å‘è´Ÿè´£äººè¯´ï¼Œ"ä¸»åº”ç”¨å†™æ—¥å¿—åˆ°å…±äº«å·ï¼ŒSidecar å®¹å™¨è¯»å–å¹¶è½¬å‘ã€‚"

å¤šå®¹å™¨ Pod ä¸­çš„å®¹å™¨å…±äº«ç½‘ç»œå’Œå­˜å‚¨ï¼Œéå¸¸é€‚åˆè¿™ç§åœºæ™¯ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Multi-container Pods" å’Œ "Logging Architecture" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-multicontainer-pod',
            description: 'åˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå®¹å™¨çš„ Pod',
            hint: 'åœ¨ containers æ•°ç»„ä¸­å®šä¹‰å¤šä¸ªå®¹å™¨',
            checkCondition: (state) => state.pods.some(p => p.spec.containers.length >= 2)
        },
        {
            id: 'shared-volume',
            description: 'é…ç½®å…±äº« Volume è®©ä¸¤ä¸ªå®¹å™¨å¯ä»¥äº¤æ¢æ•°æ®',
            hint: 'ä½¿ç”¨ emptyDir ç±»å‹çš„ volumeï¼Œä¸¤ä¸ªå®¹å™¨éƒ½æŒ‚è½½å®ƒ',
            checkCondition: (state) => state.pods.some(p =>
                p.spec.volumes && p.spec.volumes.some(v => v.emptyDir) &&
                p.spec.containers.filter(c => c.volumeMounts && c.volumeMounts.length > 0).length >= 2
            )
        }
    ],
    hints: [
        'ä¸€ä¸ª Pod å¯ä»¥æœ‰å¤šä¸ª containers',
        'emptyDir: {} åˆ›å»ºä¸´æ—¶å…±äº«å­˜å‚¨',
        'æ¯ä¸ªå®¹å™¨é€šè¿‡ volumeMounts æŒ‚è½½åŒä¸€ä¸ª volume',
        'å®¹å™¨ä¹‹é—´å¯ä»¥ç”¨ localhost é€šä¿¡'
    ],
    rewards: { xp: 150 }
};

const scenario11_2: Scenario = {
    id: '11-2',
    title: 'InitContainer åˆå§‹åŒ–å®¹å™¨',
    description: 'ä½¿ç”¨åˆå§‹åŒ–å®¹å™¨è¿›è¡Œé¢„å¤„ç†',
    story: `æ–°çš„å¾®æœåŠ¡åœ¨å¯åŠ¨å‰éœ€è¦ç­‰å¾…æ•°æ®åº“å°±ç»ªã€‚

"åº”ç”¨å®¹å™¨å¯åŠ¨æ—¶æ•°æ®åº“è¿˜æ²¡å‡†å¤‡å¥½ï¼Œå¯¼è‡´è¿æ¥å¤±è´¥ï¼Œ" å¼€å‘è€…æŠ±æ€¨é“ï¼Œ"æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæœºåˆ¶æ¥ç¡®ä¿ä¾èµ–æœåŠ¡å…ˆå°±ç»ªã€‚"

Init Containers åœ¨åº”ç”¨å®¹å™¨ä¹‹å‰è¿è¡Œï¼Œå¯ä»¥ç”¨æ¥ç­‰å¾…ä¾èµ–ã€ä¸‹è½½é…ç½®ç­‰ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Init Containers" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-initcontainer',
            description: 'åˆ›å»ºä¸€ä¸ª Podï¼ŒåŒ…å« Init Container ç”¨äºåˆå§‹åŒ–',
            hint: 'åœ¨ Pod spec ä¸­æ·»åŠ  initContainers æ•°ç»„',
            checkCondition: (state) => state.pods.some(p => 
                p.spec.initContainers && p.spec.initContainers.length > 0
            )
        },
        {
            id: 'verify-init',
            description: 'è§‚å¯Ÿ Pod çŠ¶æ€å˜åŒ–ï¼Œç¡®è®¤ Init Container å…ˆæ‰§è¡Œ',
            hint: 'Init é˜¶æ®µ Pod çŠ¶æ€ä¼šæ˜¾ç¤º Init:x/y',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(pods?|po)/.test(cmd))
        }
    ],
    hints: [
        'initContainers ä¸ containers åŒçº§',
        'Init Containers æŒ‰é¡ºåºæ‰§è¡Œï¼Œå…¨éƒ¨æˆåŠŸåæ‰å¯åŠ¨ä¸»å®¹å™¨',
        'å¸¸ç”¨äºï¼šç­‰å¾…æœåŠ¡å°±ç»ªã€ä¸‹è½½é…ç½®ã€è®¾ç½®æƒé™',
        'å¯ä»¥ä½¿ç”¨ busybox é•œåƒé…åˆ sleep æˆ– wget å‘½ä»¤'
    ],
    rewards: { xp: 140 }
};

// ========== ç¬¬åäºŒç« ï¼šå¥åº·æ£€æŸ¥ ==========

const scenario12_1: Scenario = {
    id: '12-1',
    title: 'Liveness ä¸ Readiness æ¢é’ˆ',
    description: 'é…ç½®å®¹å™¨å¥åº·æ£€æŸ¥ï¼Œå®ç°è‡ªåŠ¨æ•…éšœæ£€æµ‹ä¸æ¢å¤',
    story: `å‡Œæ™¨å‘Šè­¦ï¼šAPI æœåŠ¡æ— å“åº”ï¼Œä½† Pod çŠ¶æ€æ˜¾ç¤º Runningã€‚

"å®¹å™¨è¿›ç¨‹è¿˜åœ¨ï¼Œä½†å†…éƒ¨å·²ç»æ­»é”äº†ï¼Œ" SRE åˆ†æé“ï¼Œ"Kubernetes çœ‹ä¸å‡ºé—®é¢˜ï¼Œæ‰€ä»¥ä¸ä¼šé‡å¯å®ƒã€‚"

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¥åº·æ£€æŸ¥ï¼š
- **Liveness Probe**ï¼šæ£€æµ‹å®¹å™¨æ˜¯å¦"æ´»ç€"ï¼Œå¤±è´¥åˆ™é‡å¯å®¹å™¨
- **Readiness Probe**ï¼šæ£€æµ‹å®¹å™¨æ˜¯å¦"å°±ç»ª"ï¼Œå¤±è´¥åˆ™ä» Service ç§»é™¤

ä½ éœ€è¦ä¸ºåº”ç”¨é…ç½®åˆé€‚çš„æ¢é’ˆï¼Œè®© Kubernetes èƒ½è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†æ•…éšœã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ "Configure Liveness, Readiness and Startup Probes"ã€‚`,
    difficulty: 'medium',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { name: 'api-server', namespace: 'default' },
                spec: { 
                    replicas: 2, 
                    selector: { matchLabels: { app: 'api' } },
                    template: {
                        metadata: { labels: { app: 'api' } },
                        spec: { containers: [{ name: 'api', image: 'myapi:v1', ports: [{ containerPort: 8080 }] }] }
                    }
                },
                status: { replicas: 2, readyReplicas: 2, availableReplicas: 2 }
            }
        ],
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'api-server-abc', namespace: 'default', labels: { app: 'api' } },
                spec: { containers: [{ name: 'api', image: 'myapi:v1' }], nodeName: 'node01' },
                status: { phase: 'Running', podIP: '10.244.1.30', hostIP: '192.168.1.3' }
            },
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'api-server-def', namespace: 'default', labels: { app: 'api' } },
                spec: { containers: [{ name: 'api', image: 'myapi:v1' }], nodeName: 'node02' },
                status: { phase: 'Running', podIP: '10.244.2.31', hostIP: '192.168.1.4' }
            }
        ]
    },
    objectives: [
        {
            id: 'check-current',
            description: 'æŸ¥çœ‹å½“å‰ Deployment é…ç½®ï¼Œç¡®è®¤æ²¡æœ‰å¥åº·æ£€æŸ¥',
            hint: 'ä½¿ç”¨ kubectl describe deploy æˆ– get -o yaml',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(describe\s+deploy|get\s+deploy.*-o\s+(yaml|json))/.test(cmd)
            )
        },
        {
            id: 'create-pod-with-probes',
            description: 'åˆ›å»º/æ›´æ–° Podï¼Œé…ç½® livenessProbeï¼ˆHTTP æˆ– TCP æˆ– execï¼‰',
            hint: 'åœ¨ containers ä¸­æ·»åŠ  livenessProbe é…ç½®',
            checkCondition: (state) => state.pods.some(p =>
                p.spec.containers.some(c => c.livenessProbe)
            )
        },
        {
            id: 'add-readiness',
            description: 'åŒæ—¶é…ç½® readinessProbeï¼Œç¡®ä¿æœåŠ¡å°±ç»ªåæ‰æ¥æ”¶æµé‡',
            hint: 'readinessProbe å¯ç”¨äºæ£€æµ‹ä¾èµ–æœåŠ¡æ˜¯å¦å°±ç»ª',
            checkCondition: (state) => state.pods.some(p =>
                p.spec.containers.some(c => c.readinessProbe)
            )
        },
        {
            id: 'verify-probes',
            description: 'ä½¿ç”¨ describe ç¡®è®¤æ¢é’ˆé…ç½®æ­£ç¡®',
            hint: 'æŸ¥çœ‹ Liveness å’Œ Readiness å­—æ®µ',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+describe\s+pod/.test(cmd))
        }
    ],
    hints: [
        'livenessProbe.httpGet: {path: "/healthz", port: 8080}',
        'livenessProbe.tcpSocket: {port: 8080}',
        'livenessProbe.exec: {command: ["cat", "/tmp/healthy"]}',
        'å…³é”®å‚æ•°: initialDelaySeconds(å¯åŠ¨ç­‰å¾…), periodSeconds(æ£€æŸ¥é—´éš”), failureThreshold(å¤±è´¥é˜ˆå€¼)'
    ],
    rewards: { xp: 160 }
};

// ========== ç¬¬åä¸‰ç« ï¼šæ‰¹å¤„ç†ä»»åŠ¡ ==========

const scenario13_1: Scenario = {
    id: '13-1',
    title: 'Job ä¸€æ¬¡æ€§ä»»åŠ¡',
    description: 'ä½¿ç”¨ Job è¿è¡Œä¸€æ¬¡æ€§æ‰¹å¤„ç†ä»»åŠ¡',
    story: `æ•°æ®å›¢é˜Ÿéœ€è¦è¿è¡Œä¸€ä¸ªæ•°æ®è¿ç§»è„šæœ¬ã€‚

"è¿™ä¸ªä»»åŠ¡åªéœ€è¦æ‰§è¡Œä¸€æ¬¡ï¼Œæ‰§è¡Œå®Œå°±ç»“æŸï¼Œ" æ•°æ®å·¥ç¨‹å¸ˆè¯´ï¼Œ"è€Œä¸”å¦‚æœå¤±è´¥äº†éœ€è¦èƒ½è‡ªåŠ¨é‡è¯•ã€‚"

Job èµ„æºä¸“é—¨ç”¨äºè¿è¡Œä¸€æ¬¡æ€§ä»»åŠ¡ï¼Œå®Œæˆå Pod ä¸ä¼šé‡å¯ã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Jobs" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-job',
            description: 'åˆ›å»ºä¸€ä¸ª Job èµ„æº',
            hint: 'Job çš„ kind æ˜¯ Jobï¼ŒapiVersion æ˜¯ batch/v1',
            checkCondition: (state) => state.jobs.length > 0
        },
        {
            id: 'check-job-status',
            description: 'æŸ¥çœ‹ Job æ‰§è¡ŒçŠ¶æ€',
            hint: 'ä½¿ç”¨ kubectl get jobs æŸ¥çœ‹å®Œæˆæƒ…å†µ',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+jobs?/.test(cmd))
        }
    ],
    hints: [
        'restartPolicy å¿…é¡»æ˜¯ Never æˆ– OnFailure',
        'backoffLimit æ§åˆ¶å¤±è´¥é‡è¯•æ¬¡æ•°',
        'completions æŒ‡å®šéœ€è¦æˆåŠŸå®Œæˆçš„ Pod æ•°',
        'parallelism æ§åˆ¶å¹¶è¡Œæ‰§è¡Œçš„ Pod æ•°'
    ],
    rewards: { xp: 130 }
};

const scenario13_2: Scenario = {
    id: '13-2',
    title: 'CronJob å®šæ—¶ä»»åŠ¡',
    description: 'ä½¿ç”¨ CronJob è¿è¡Œå®šæœŸæ‰§è¡Œçš„ä»»åŠ¡',
    story: `è¿ç»´å›¢é˜Ÿéœ€è¦æ¯å¤©å‡Œæ™¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶ã€‚

"æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œæ¯å¤©è‡ªåŠ¨æ‰§è¡Œæ¸…ç†è„šæœ¬ï¼Œ" è¿ç»´å·¥ç¨‹å¸ˆè¯´ã€‚

CronJob å¯ä»¥æŒ‰ç…§ Cron è¡¨è¾¾å¼å®šæœŸåˆ›å»º Jobã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "CronJob" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'create-cronjob',
            description: 'åˆ›å»ºä¸€ä¸ª CronJobï¼Œè®¾ç½®å®šæ—¶è°ƒåº¦è§„åˆ™',
            hint: 'ä½¿ç”¨ schedule å­—æ®µå®šä¹‰ Cron è¡¨è¾¾å¼',
            checkCondition: (state) => state.cronJobs.length > 0
        },
        {
            id: 'verify-cronjob',
            description: 'æŸ¥çœ‹ CronJob çš„è°ƒåº¦é…ç½®',
            hint: 'ä½¿ç”¨ kubectl get cronjobs æˆ– describe',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+(get|describe)\s+cronjobs?/.test(cmd))
        }
    ],
    hints: [
        'schedule æ ¼å¼: "åˆ† æ—¶ æ—¥ æœˆ å‘¨"',
        'ä¾‹å¦‚ "0 2 * * *" è¡¨ç¤ºæ¯å¤©å‡Œæ™¨ 2 ç‚¹',
        'concurrencyPolicy: Allow/Forbid/Replace',
        'successfulJobsHistoryLimit ä¿ç•™æˆåŠŸ Job å†å²æ•°é‡'
    ],
    rewards: { xp: 140 }
};

// ========== ç¬¬åå››ç« ï¼šå‡çº§ä¸å›æ»š ==========

const scenario14_1: Scenario = {
    id: '14-1',
    title: 'Deployment æ»šåŠ¨æ›´æ–°',
    description: 'æ‰§è¡Œ Deployment çš„æ»šåŠ¨æ›´æ–°',
    story: `æ–°ç‰ˆæœ¬åº”ç”¨å·²ç»å‡†å¤‡å¥½å‘å¸ƒäº†ã€‚

"æˆ‘ä»¬éœ€è¦åšåˆ°é›¶åœæœºæ›´æ–°ï¼Œ" äº§å“ç»ç†è¦æ±‚ï¼Œ"ç”¨æˆ·ä¸èƒ½æ„ŸçŸ¥åˆ°æœåŠ¡ä¸­æ–­ã€‚"

Deployment çš„æ»šåŠ¨æ›´æ–°ç­–ç•¥å¯ä»¥é€æ­¥æ›¿æ¢æ—§ç‰ˆæœ¬ Podã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Performing a Rolling Update" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    objectives: [
        {
            id: 'update-image',
            description: 'æ›´æ–° Deployment çš„å®¹å™¨é•œåƒç‰ˆæœ¬',
            hint: 'å¯ä»¥ä½¿ç”¨ kubectl set image å‘½ä»¤æˆ–ç¼–è¾‘ YAML',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(set\s+image|apply|edit)/.test(cmd)
            )
        },
        {
            id: 'watch-rollout',
            description: 'è§‚å¯Ÿæ»šåŠ¨æ›´æ–°è¿›åº¦',
            hint: 'ä½¿ç”¨ rollout status å‘½ä»¤',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+rollout\s+status/.test(cmd))
        },
        {
            id: 'check-history',
            description: 'æŸ¥çœ‹ Deployment çš„æ›´æ–°å†å²',
            hint: 'ä½¿ç”¨ rollout history å‘½ä»¤',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+rollout\s+history/.test(cmd))
        }
    ],
    hints: [
        'kubectl set image deployment/<name> <container>=<image>',
        'kubectl rollout status deployment/<name> è§‚å¯Ÿè¿›åº¦',
        'kubectl rollout history deployment/<name> æŸ¥çœ‹å†å²',
        'è®°å½•å˜æ›´åŸå› : --record å‚æ•°'
    ],
    rewards: { xp: 150 }
};

const scenario14_2: Scenario = {
    id: '14-2',
    title: 'Deployment å›æ»š',
    description: 'å°† Deployment å›æ»šåˆ°ä¹‹å‰çš„ç‰ˆæœ¬',
    story: `ç³Ÿç³•ï¼æ–°ç‰ˆæœ¬æœ‰ä¸¥é‡ bugï¼Œéœ€è¦ç´§æ€¥å›æ»šï¼

"å¿«å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬ï¼" å€¼ç­ç»ç†ç„¦æ€¥åœ°è¯´ã€‚

å¹¸å¥½ Deployment ä¿å­˜äº†ç‰ˆæœ¬å†å²ï¼Œå¯ä»¥å¿«é€Ÿå›æ»šã€‚

ğŸ’¡ æç¤ºï¼šæŸ¥é˜… Kubernetes æ–‡æ¡£ä¸­ "Rolling Back a Deployment" ç« èŠ‚ã€‚`,
    difficulty: 'medium',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { name: 'buggy-app', namespace: 'default' },
                spec: { 
                    replicas: 3, 
                    selector: { matchLabels: { app: 'buggy' } },
                    template: {
                        metadata: { labels: { app: 'buggy' } },
                        spec: { containers: [{ name: 'app', image: 'myapp:v2-buggy' }] }
                    }
                },
                status: { replicas: 3, readyReplicas: 0, availableReplicas: 0 }
            }
        ]
    },
    objectives: [
        {
            id: 'check-history',
            description: 'æŸ¥çœ‹ buggy-app çš„ç‰ˆæœ¬å†å²',
            hint: 'ä½¿ç”¨ rollout history å‘½ä»¤',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+rollout\s+history/.test(cmd))
        },
        {
            id: 'rollback',
            description: 'æ‰§è¡Œå›æ»šæ“ä½œ',
            hint: 'ä½¿ç”¨ rollout undo å‘½ä»¤',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+rollout\s+undo/.test(cmd))
        },
        {
            id: 'verify-rollback',
            description: 'éªŒè¯å›æ»šæˆåŠŸ',
            hint: 'æ£€æŸ¥ Deployment å’Œ Pod çŠ¶æ€',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+(deploy|pods?)/.test(cmd))
        }
    ],
    hints: [
        'kubectl rollout undo deployment/<name> å›æ»šåˆ°ä¸Šä¸€ç‰ˆæœ¬',
        'kubectl rollout undo deployment/<name> --to-revision=N å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬',
        'å›æ»šåç”¨ get pods ç¡®è®¤ Pod é•œåƒç‰ˆæœ¬',
        'describe deployment æŸ¥çœ‹å½“å‰é•œåƒ'
    ],
    rewards: { xp: 160 }
};

// ========== ç¬¬åäº”ç« ï¼šNamespace ç®¡ç† ==========

const scenario15_1: Scenario = {
    id: '15-1',
    title: 'Namespace éš”ç¦»',
    description: 'å­¦ä¹ ä½¿ç”¨ Namespace è¿›è¡Œèµ„æºéš”ç¦»',
    story: `å…¬å¸å†³å®šå°†å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²åœ¨åŒä¸€ä¸ªé›†ç¾¤ä¸­ã€‚

"æˆ‘ä»¬éœ€è¦ä½¿ç”¨ Namespace æ¥éš”ç¦»ä¸åŒç¯å¢ƒçš„èµ„æºã€‚" æ¶æ„å¸ˆè¯´é“ã€‚

"å…ˆåˆ›å»º dev å’Œ prod ä¸¤ä¸ªå‘½åç©ºé—´ï¼Œç„¶ååœ¨ä¸åŒå‘½åç©ºé—´ä¸­éƒ¨ç½²åº”ç”¨ã€‚"`,
    difficulty: 'easy',
    objectives: [
        {
            id: 'create-dev-ns',
            description: 'åˆ›å»º dev å‘½åç©ºé—´',
            hint: 'ä½¿ç”¨ kubectl create namespace dev',
            checkCondition: (state) => state.namespaces.includes('dev')
        },
        {
            id: 'create-prod-ns',
            description: 'åˆ›å»º prod å‘½åç©ºé—´',
            hint: 'ä½¿ç”¨ kubectl create namespace prod',
            checkCondition: (state) => state.namespaces.includes('prod')
        },
        {
            id: 'deploy-to-dev',
            description: 'åœ¨ dev å‘½åç©ºé—´ä¸­éƒ¨ç½²ä¸€ä¸ª nginx Pod',
            hint: 'ä½¿ç”¨ kubectl run nginx --image=nginx -n dev',
            checkCondition: (state) => state.pods.some(p => p.metadata.namespace === 'dev')
        },
        {
            id: 'list-pods-ns',
            description: 'æŸ¥çœ‹ dev å‘½åç©ºé—´ä¸­çš„ Pod',
            hint: 'ä½¿ç”¨ kubectl get pods -n dev',
            checkCondition: (_state, history) => history.some(cmd => /kubectl\s+get\s+pods?\s+(-n|--namespace)\s+dev/.test(cmd))
        }
    ],
    hints: [
        'kubectl create namespace <name> åˆ›å»ºå‘½åç©ºé—´',
        'kubectl get ns æŸ¥çœ‹æ‰€æœ‰å‘½åç©ºé—´',
        '-n <namespace> æŒ‡å®šå‘½åç©ºé—´',
        'kubectl get pods --all-namespaces æŸ¥çœ‹æ‰€æœ‰å‘½åç©ºé—´çš„ Pod'
    ],
    rewards: { xp: 100 }
};

const scenario15_2: Scenario = {
    id: '15-2',
    title: 'ResourceQuota èµ„æºé…é¢',
    description: 'ä¸º Namespace è®¾ç½®èµ„æºé…é¢é™åˆ¶',
    story: `"å¼€å‘å›¢é˜Ÿçš„ Pod å¤ªå¤šäº†ï¼Œå ç”¨äº†å¤§é‡é›†ç¾¤èµ„æºã€‚" è¿ç»´è´Ÿè´£äººè¯´ã€‚

"æˆ‘ä»¬éœ€è¦ä¸º dev å‘½åç©ºé—´è®¾ç½®èµ„æºé…é¢ï¼Œé™åˆ¶ä»–ä»¬èƒ½åˆ›å»ºçš„ Pod æ•°é‡å’Œèµ„æºä½¿ç”¨é‡ã€‚"`,
    difficulty: 'medium',
    initialState: {
        namespaces: ['default', 'kube-system', 'kube-public', 'kube-node-lease', 'dev', 'prod']
    },
    objectives: [
        {
            id: 'create-quota',
            description: 'ä¸º dev å‘½åç©ºé—´åˆ›å»º ResourceQuotaï¼Œé™åˆ¶æœ€å¤š 5 ä¸ª Pod',
            hint: 'åˆ›å»ºä¸€ä¸ª ResourceQuota YAML æ–‡ä»¶å¹¶ apply',
            checkCondition: (state) => state.resourceQuotas.some(q => 
                q.metadata.namespace === 'dev' && q.spec.hard?.pods
            )
        },
        {
            id: 'view-quota',
            description: 'æŸ¥çœ‹ dev å‘½åç©ºé—´çš„èµ„æºé…é¢ä½¿ç”¨æƒ…å†µ',
            hint: 'ä½¿ç”¨ kubectl describe resourcequota -n dev',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(describe|get)\s+resourcequota/.test(cmd)
            )
        }
    ],
    hints: [
        'ResourceQuota å¯ä»¥é™åˆ¶ pods, requests.cpu, requests.memory, limits.cpu, limits.memory ç­‰',
        'kubectl create quota my-quota --hard=pods=5 -n dev',
        'kubectl describe quota -n dev æŸ¥çœ‹é…é¢ä½¿ç”¨æƒ…å†µ',
        'è¶…å‡ºé…é¢çš„èµ„æºåˆ›å»ºè¯·æ±‚ä¼šè¢«æ‹’ç»'
    ],
    rewards: { xp: 120 }
};

// ========== ç¬¬åå…­ç« ï¼šåŠ¨æ€å­˜å‚¨ä¾›ç»™ ==========

const scenario16_1: Scenario = {
    id: '16-1',
    title: 'StorageClass ä¸åŠ¨æ€ä¾›ç»™',
    description: 'å­¦ä¹ ä½¿ç”¨ StorageClass å®ç°åŠ¨æ€å­˜å‚¨ä¾›ç»™',
    story: `"æ‰‹åŠ¨åˆ›å»º PV å¤ªéº»çƒ¦äº†ï¼" å¼€å‘å›¢é˜ŸæŠ±æ€¨é“ã€‚

"æˆ‘ä»¬å¯ä»¥é…ç½® StorageClassï¼Œè®© PVC è‡ªåŠ¨åˆ›å»º PVã€‚" å­˜å‚¨ç®¡ç†å‘˜è§£é‡Šé“ã€‚

"è¿™å°±æ˜¯åŠ¨æ€ä¾›ç»™ï¼ˆDynamic Provisioningï¼‰çš„é­”åŠ›ã€‚"`,
    difficulty: 'medium',
    initialState: {
        storageClasses: [
            {
                apiVersion: 'storage.k8s.io/v1',
                kind: 'StorageClass',
                metadata: { name: 'standard', annotations: { 'storageclass.kubernetes.io/is-default-class': 'true' } },
                provisioner: 'kubernetes.io/no-provisioner',
                reclaimPolicy: 'Delete',
                volumeBindingMode: 'WaitForFirstConsumer'
            },
            {
                apiVersion: 'storage.k8s.io/v1',
                kind: 'StorageClass',
                metadata: { name: 'fast-ssd' },
                provisioner: 'kubernetes.io/gce-pd',
                parameters: { type: 'pd-ssd' },
                reclaimPolicy: 'Retain',
                volumeBindingMode: 'Immediate'
            }
        ]
    },
    objectives: [
        {
            id: 'list-sc',
            description: 'æŸ¥çœ‹é›†ç¾¤ä¸­çš„ StorageClass',
            hint: 'ä½¿ç”¨ kubectl get storageclass',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(storageclass|sc)/.test(cmd)
            )
        },
        {
            id: 'create-pvc-dynamic',
            description: 'åˆ›å»ºä¸€ä¸ªä½¿ç”¨ fast-ssd StorageClass çš„ PVC',
            hint: 'åœ¨ PVC spec ä¸­æŒ‡å®š storageClassName: fast-ssd',
            checkCondition: (state) => state.persistentVolumeClaims.some(pvc => 
                pvc.spec.storageClassName === 'fast-ssd'
            )
        },
        {
            id: 'verify-pv-created',
            description: 'éªŒè¯ PV æ˜¯å¦è¢«è‡ªåŠ¨åˆ›å»º',
            hint: 'ä½¿ç”¨ kubectl get pv æŸ¥çœ‹',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(pv|persistentvolumes?)/.test(cmd)
            )
        }
    ],
    hints: [
        'kubectl get sc æŸ¥çœ‹ StorageClass',
        'storageClassName åœ¨ PVC ä¸­æŒ‡å®šä½¿ç”¨å“ªä¸ª StorageClass',
        'é»˜è®¤ StorageClass æ ‡è®°ä¸º (default)',
        'reclaimPolicy å†³å®š PV åˆ é™¤åçš„è¡Œä¸ºï¼šDelete æˆ– Retain',
        'volumeBindingMode: WaitForFirstConsumer ç­‰åˆ° Pod ä½¿ç”¨æ—¶æ‰ç»‘å®š'
    ],
    rewards: { xp: 140 }
};

// ========== ç¬¬åä¸ƒç« ï¼šHPA è‡ªåŠ¨æ‰©ç¼©å®¹ ==========

const scenario17_1: Scenario = {
    id: '17-1',
    title: 'Horizontal Pod Autoscaler',
    description: 'é…ç½® HPA å®ç° Pod è‡ªåŠ¨æ‰©ç¼©å®¹',
    story: `"æˆ‘ä»¬çš„ web æœåŠ¡æµé‡æ³¢åŠ¨å¾ˆå¤§ï¼Œé«˜å³°æœŸéœ€è¦æ›´å¤šå‰¯æœ¬ã€‚" äº§å“ç»ç†è¯´ã€‚

"æ‰‹åŠ¨æ‰©å®¹å¤ªæ…¢äº†ï¼Œæˆ‘ä»¬éœ€è¦è‡ªåŠ¨æ‰©ç¼©å®¹ã€‚" 

"HPA å¯ä»¥æ ¹æ® CPU æˆ–å†…å­˜ä½¿ç”¨ç‡è‡ªåŠ¨è°ƒæ•´ Pod å‰¯æœ¬æ•°ã€‚" ä½ è§£é‡Šé“ã€‚`,
    difficulty: 'medium',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { name: 'web-app', namespace: 'default' },
                spec: {
                    replicas: 2,
                    selector: { matchLabels: { app: 'web' } },
                    template: {
                        metadata: { labels: { app: 'web' } },
                        spec: {
                            containers: [{
                                name: 'web',
                                image: 'nginx',
                                resources: {
                                    requests: { cpu: '100m', memory: '128Mi' },
                                    limits: { cpu: '500m', memory: '256Mi' }
                                }
                            }]
                        }
                    }
                },
                status: { replicas: 2, readyReplicas: 2, availableReplicas: 2 }
            }
        ],
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'web-app-abc123', namespace: 'default', labels: { app: 'web' } },
                spec: { containers: [{ name: 'web', image: 'nginx' }], nodeName: 'node01' },
                status: { phase: 'Running', podIP: '10.244.1.50', hostIP: '192.168.1.3' }
            },
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'web-app-def456', namespace: 'default', labels: { app: 'web' } },
                spec: { containers: [{ name: 'web', image: 'nginx' }], nodeName: 'node02' },
                status: { phase: 'Running', podIP: '10.244.2.50', hostIP: '192.168.1.4' }
            }
        ]
    },
    objectives: [
        {
            id: 'create-hpa',
            description: 'ä¸º web-app Deployment åˆ›å»º HPAï¼Œç›®æ ‡ CPU ä½¿ç”¨ç‡ 50%ï¼Œæœ€å° 2 å‰¯æœ¬ï¼Œæœ€å¤§ 10 å‰¯æœ¬',
            hint: 'ä½¿ç”¨ kubectl autoscale deployment web-app --cpu=50% --min=2 --max=10',
            checkCondition: (state) => state.hpas.some(h => 
                h.spec.scaleTargetRef.name === 'web-app' &&
                h.spec.minReplicas === 2 &&
                h.spec.maxReplicas === 10
            )
        },
        {
            id: 'view-hpa',
            description: 'æŸ¥çœ‹ HPA çŠ¶æ€',
            hint: 'ä½¿ç”¨ kubectl get hpa',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(hpa|horizontalpodautoscaler)/.test(cmd)
            )
        },
        {
            id: 'describe-hpa',
            description: 'æŸ¥çœ‹ HPA è¯¦ç»†ä¿¡æ¯',
            hint: 'ä½¿ç”¨ kubectl describe hpa web-app',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+describe\s+(hpa|horizontalpodautoscaler)/.test(cmd)
            )
        }
    ],
    hints: [
        'kubectl autoscale deployment <name> --cpu=<ç›®æ ‡CPU%> --min=<æœ€å°å‰¯æœ¬> --max=<æœ€å¤§å‰¯æœ¬>',
        'HPA éœ€è¦ metrics-server æä¾›æŒ‡æ ‡',
        'kubectl top pods æŸ¥çœ‹ Pod èµ„æºä½¿ç”¨æƒ…å†µ',
        'HPA ä¼šæ ¹æ®å½“å‰è´Ÿè½½è‡ªåŠ¨è°ƒæ•´å‰¯æœ¬æ•°'
    ],
    rewards: { xp: 150 }
};

const scenario17_2: Scenario = {
    id: '17-2',
    title: 'HPA é«˜çº§é…ç½®',
    description: 'ä½¿ç”¨ YAML é…ç½®æ›´å¤æ‚çš„ HPA ç­–ç•¥',
    story: `"é™¤äº† CPUï¼Œæˆ‘ä»¬è¿˜æƒ³æ ¹æ®å†…å­˜ä½¿ç”¨ç‡æ¥æ‰©ç¼©å®¹ã€‚" å¼€å‘è´Ÿè´£äººè¯´ã€‚

"è€Œä¸”æ‰©å®¹è¦å¿«ï¼Œç¼©å®¹è¦æ…¢ï¼Œé¿å…é¢‘ç¹æ³¢åŠ¨ã€‚"

ä½ å†³å®šä½¿ç”¨ YAML æ¥é…ç½®æ›´ç²¾ç»†çš„ HPA ç­–ç•¥ã€‚`,
    difficulty: 'hard',
    initialState: {
        deployments: [
            {
                apiVersion: 'apps/v1',
                kind: 'Deployment',
                metadata: { name: 'api-server', namespace: 'default' },
                spec: {
                    replicas: 3,
                    selector: { matchLabels: { app: 'api' } },
                    template: {
                        metadata: { labels: { app: 'api' } },
                        spec: {
                            containers: [{
                                name: 'api',
                                image: 'myapi:v1',
                                resources: {
                                    requests: { cpu: '200m', memory: '256Mi' },
                                    limits: { cpu: '1', memory: '512Mi' }
                                }
                            }]
                        }
                    }
                },
                status: { replicas: 3, readyReplicas: 3, availableReplicas: 3 }
            }
        ]
    },
    objectives: [
        {
            id: 'create-hpa-yaml',
            description: 'åˆ›å»ºä¸€ä¸ª HPA YAML æ–‡ä»¶ï¼Œé…ç½® CPU å’Œ Memory åŒæŒ‡æ ‡',
            hint: 'ä½¿ç”¨ vim hpa.yaml åˆ›å»ºæ–‡ä»¶',
            checkCondition: (_state, history) => history.some(cmd => 
                /vim\s+.*hpa.*\.yaml/.test(cmd) || /cat.*hpa.*\.yaml/.test(cmd)
            )
        },
        {
            id: 'apply-hpa',
            description: 'åº”ç”¨ HPA é…ç½®',
            hint: 'ä½¿ç”¨ kubectl apply -f hpa.yaml',
            checkCondition: (state) => state.hpas.some(h => 
                h.spec.scaleTargetRef.name === 'api-server' &&
                h.spec.metrics && h.spec.metrics.length > 1
            )
        }
    ],
    hints: [
        'HPA v2 æ”¯æŒå¤šä¸ª metrics é…ç½®',
        'behavior å­—æ®µå¯é…ç½®æ‰©ç¼©å®¹è¡Œä¸º',
        'scaleDown.stabilizationWindowSeconds é˜²æ­¢é¢‘ç¹ç¼©å®¹',
        'scaleUp.policies å¯è®¾ç½®æ‰©å®¹é€Ÿç‡é™åˆ¶'
    ],
    rewards: { xp: 180 }
};

// ========== ç¬¬åå…«ç« ï¼šIngress å…¥å£æ§åˆ¶ ==========

const scenario18_1: Scenario = {
    id: '18-1',
    title: 'Ingress åŸºç¡€',
    description: 'ä½¿ç”¨ Ingress æš´éœ² HTTP æœåŠ¡',
    story: `"æˆ‘ä»¬æœ‰å¤šä¸ªå¾®æœåŠ¡ï¼Œæ¯ä¸ªéƒ½ç”¨ NodePort å¤ªä¹±äº†ã€‚" è¿ç»´è¯´ã€‚

"Ingress å¯ä»¥åœ¨ä¸€ä¸ªå…¥å£ç‚¹æš´éœ²å¤šä¸ªæœåŠ¡ï¼Œè¿˜æ”¯æŒåŸŸåå’Œè·¯å¾„è·¯ç”±ã€‚"

"å°±åƒä¸€ä¸ªæ™ºèƒ½çš„åå‘ä»£ç†ï¼Œæ ¹æ®è¯·æ±‚è·¯ç”±åˆ°ä¸åŒçš„åç«¯æœåŠ¡ã€‚"`,
    difficulty: 'medium',
    initialState: {
        services: [
            {
                apiVersion: 'v1',
                kind: 'Service',
                metadata: { name: 'frontend-svc', namespace: 'default' },
                spec: {
                    type: 'ClusterIP',
                    selector: { app: 'frontend' },
                    ports: [{ port: 80, targetPort: 3000 }]
                }
            },
            {
                apiVersion: 'v1',
                kind: 'Service',
                metadata: { name: 'api-svc', namespace: 'default' },
                spec: {
                    type: 'ClusterIP',
                    selector: { app: 'api' },
                    ports: [{ port: 80, targetPort: 8080 }]
                }
            }
        ],
        pods: [
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'frontend-pod', namespace: 'default', labels: { app: 'frontend' } },
                spec: { containers: [{ name: 'frontend', image: 'frontend:v1' }], nodeName: 'node01' },
                status: { phase: 'Running', podIP: '10.244.1.60' }
            },
            {
                apiVersion: 'v1',
                kind: 'Pod',
                metadata: { name: 'api-pod', namespace: 'default', labels: { app: 'api' } },
                spec: { containers: [{ name: 'api', image: 'api:v1' }], nodeName: 'node02' },
                status: { phase: 'Running', podIP: '10.244.2.60' }
            }
        ]
    },
    objectives: [
        {
            id: 'view-services',
            description: 'æŸ¥çœ‹ç°æœ‰çš„ Service',
            hint: 'ä½¿ç”¨ kubectl get svc',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+get\s+(services?|svc)/.test(cmd)
            )
        },
        {
            id: 'create-ingress',
            description: 'åˆ›å»º Ingressï¼Œå°† / è·¯ç”±åˆ° frontend-svcï¼Œ/api è·¯ç”±åˆ° api-svc',
            hint: 'åˆ›å»º Ingress YAML å¹¶ apply',
            checkCondition: (state) => state.ingresses.some(ing => 
                ing.spec.rules?.some(r => r.http?.paths?.length && r.http.paths.length >= 2)
            )
        },
        {
            id: 'view-ingress',
            description: 'æŸ¥çœ‹ Ingress é…ç½®',
            hint: 'ä½¿ç”¨ kubectl get ingress æˆ– kubectl describe ingress',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+(get|describe)\s+(ingress|ing)/.test(cmd)
            )
        }
    ],
    hints: [
        'Ingress éœ€è¦ Ingress Controllerï¼ˆå¦‚ nginx-ingressï¼‰æ‰èƒ½å·¥ä½œ',
        'spec.rules å®šä¹‰è·¯ç”±è§„åˆ™',
        'path æ”¯æŒ Prefix å’Œ Exact åŒ¹é…ç±»å‹',
        'kubectl get ing æ˜¯ kubectl get ingress çš„ç®€å†™'
    ],
    rewards: { xp: 150 }
};

const scenario18_2: Scenario = {
    id: '18-2',
    title: 'Ingress TLS é…ç½®',
    description: 'ä¸º Ingress é…ç½® HTTPS',
    story: `"æˆ‘ä»¬çš„ç½‘ç«™éœ€è¦ HTTPSï¼" å®‰å…¨å›¢é˜Ÿå¼ºè°ƒã€‚

"Ingress æ”¯æŒ TLS ç»ˆæ­¢ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºåŒ…å«è¯ä¹¦çš„ Secretï¼Œç„¶ååœ¨ Ingress ä¸­å¼•ç”¨å®ƒã€‚"`,
    difficulty: 'hard',
    initialState: {
        services: [
            {
                apiVersion: 'v1',
                kind: 'Service',
                metadata: { name: 'web-svc', namespace: 'default' },
                spec: {
                    type: 'ClusterIP',
                    selector: { app: 'web' },
                    ports: [{ port: 80, targetPort: 8080 }]
                }
            }
        ],
        ingresses: [
            {
                apiVersion: 'networking.k8s.io/v1',
                kind: 'Ingress',
                metadata: { name: 'web-ingress', namespace: 'default' },
                spec: {
                    rules: [{
                        host: 'www.example.com',
                        http: {
                            paths: [{
                                path: '/',
                                pathType: 'Prefix',
                                backend: { service: { name: 'web-svc', port: { number: 80 } } }
                            }]
                        }
                    }]
                },
                status: {
                    loadBalancer: { ingress: [{ ip: '203.0.113.10' }] }
                }
            }
        ]
    },
    objectives: [
        {
            id: 'create-tls-secret',
            description: 'åˆ›å»º TLS ç±»å‹çš„ Secretï¼ˆåŒ…å«è¯ä¹¦ï¼‰',
            hint: 'ä½¿ç”¨ kubectl create secret tls <name> --cert=<cert> --key=<key>',
            checkCondition: (state) => state.secrets.some(s => s.type === 'kubernetes.io/tls')
        },
        {
            id: 'update-ingress-tls',
            description: 'æ›´æ–° Ingressï¼Œæ·»åŠ  TLS é…ç½®',
            hint: 'åœ¨ Ingress spec ä¸­æ·»åŠ  tls å­—æ®µ',
            checkCondition: (state) => state.ingresses.some(ing => 
                ing.spec.tls && ing.spec.tls.length > 0
            )
        },
        {
            id: 'verify-ingress',
            description: 'éªŒè¯ Ingress TLS é…ç½®',
            hint: 'ä½¿ç”¨ kubectl describe ingress',
            checkCondition: (_state, history) => history.some(cmd => 
                /kubectl\s+describe\s+(ingress|ing)/.test(cmd)
            )
        }
    ],
    hints: [
        'kubectl create secret tls my-tls --cert=tls.crt --key=tls.key',
        'Ingress spec.tls é…ç½® TLS ç»ˆæ­¢',
        'tls.secretName å¼•ç”¨åŒ…å«è¯ä¹¦çš„ Secret',
        'tls.hosts æŒ‡å®šä½¿ç”¨è¯¥è¯ä¹¦çš„åŸŸå'
    ],
    rewards: { xp: 180 }
};

// ========== å¯¼å‡ºæ‰€æœ‰å…³å¡ ==========

export const allScenarios: Scenario[] = [
    scenario1_1,
    scenario1_2,
    scenario1_3,
    scenario2_1,
    scenario2_2,
    scenario3_1,
    scenario3_2,
    scenario4_1,
    scenario5_1,
    scenario6_1,
    scenario6_2,
    scenario7_1,
    scenario7_2,
    scenario8_1,
    scenario8_2,
    scenario9_1,
    scenario10_1,
    scenario11_1,
    scenario11_2,
    scenario12_1,
    scenario13_1,
    scenario13_2,
    scenario14_1,
    scenario14_2,
    // æ–°å¢å…³å¡
    scenario15_1,
    scenario15_2,
    scenario16_1,
    scenario17_1,
    scenario17_2,
    scenario18_1,
    scenario18_2,
    // CKA è®¤è¯è€ƒè¯•æ¨¡æ‹Ÿï¼ˆ16é“çœŸé¢˜ï¼‰
    ...ckaScenarios,
];

export const getScenarioById = (id: string): Scenario | undefined => {
    return allScenarios.find(s => s.id === id);
};
